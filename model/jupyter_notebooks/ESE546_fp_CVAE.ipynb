{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESE546-fp-CVAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrjsCpUNmdpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shared by all\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils # We should use this eventually.\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# For DataLoader\n",
        "from PIL import Image\n",
        "import numbers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGua5QHwbAVe",
        "colab_type": "code",
        "outputId": "9b39ecde-97d1-45df-9419-c36979770343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUC4mk_Lm-wA",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Code: `CarlaDataset.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYae0Zdym6iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class CarlaDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        # xcxc I'm assuming that the images live in _out.\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.df = self._get_dataframe()\n",
        "        self.df_as_mat = self.df.values\n",
        "    \n",
        "    def __len__(self):\n",
        "        num_rows, _ = self.df_as_mat.shape\n",
        "        return num_rows\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Generate one sample of data.\n",
        "        '''\n",
        "        # We're gonna do some hardcore hard-coding here.\n",
        "        # First, extract our control inputs\n",
        "        row = self.df_as_mat[idx, :]\n",
        "        # xcxc We're... We're not exactly doing anything with our control inputs. For now.\n",
        "        # We lop off the final value in -1 because of our dataframe- we \n",
        "        # interpret the indicator value of whether it's stationary or not \n",
        "        # as a boolean, and python interprets it as a number.\n",
        "        control_inputs = np.array(\n",
        "            [x for x in row if isinstance(x, numbers.Number)][:-1])\n",
        "        is_stationary = row[-1]\n",
        "        \n",
        "        curr_images = self._get_image_tensor_for_row(row[0], is_stationary)\n",
        "        # Get the next row\n",
        "        next_delta = 4 # xcxc This is a hardcoded parameter from Klayton's data.\n",
        "        next_input_id = int(row[0]) + next_delta\n",
        "        num_rows_next = np.sum(self.df['input_num'] == str(next_input_id))\n",
        "        if num_rows_next == 0:\n",
        "            # No next: treat it as if we're stationary\n",
        "            return (curr_images, curr_images, np.zeros(len(control_inputs)))\n",
        "        elif is_stationary == True:\n",
        "            # If it's stationary, then simply return our current images\n",
        "            return (curr_images, curr_images, np.zeros(len(control_inputs)))\n",
        "        else:\n",
        "            next_images = self._get_image_tensor_for_row(\n",
        "                str(next_input_id), is_stationary)\n",
        "            return (curr_images, next_images, control_inputs)\n",
        "    \n",
        "    def _get_image_tensor_for_row(self, row_id, is_stationary):\n",
        "        '''\n",
        "        Inputs:\n",
        "            row_id: String that represents the input_num\n",
        "        Outputs:\n",
        "            A (2 x H x W x 4) 4D matrix of the two images.\n",
        "        '''\n",
        "        # The row_id should be the input_num. Should also be a string.\n",
        "        which_row = (self.df['input_num'] == row_id)\n",
        "        where_stationary = (self.df['is_stationary'] == is_stationary)\n",
        "        row = self.df[which_row & where_stationary]\n",
        "        n_res, _ = row.shape\n",
        "        if n_res > 1:\n",
        "            # xcxc I'm assuming there's only one row per row_id.\n",
        "            # This may or may not be a strictly held invariant.\n",
        "            print(\"XCXC: THERE ARE MORE THAN 1 ROW FOR A ROW_ID\")\n",
        "        row = row.values[0]\n",
        "        images = []\n",
        "        for ele in row:\n",
        "            if str(ele).split('.')[-1] == 'png':\n",
        "                full_name = os.path.join(self.data_dir, '_out', ele)\n",
        "                np_arr = np.asarray(Image.open(full_name))\n",
        "                np_arr = self._rearrange_axes_image(np_arr)\n",
        "                # Apply transform on each image independently.\n",
        "                if self.transform:\n",
        "                    np_arr = self.transform(np_arr)\n",
        "                images.append(np_arr)\n",
        "        images = np.array(images)\n",
        "        return images\n",
        "    \n",
        "    def _rearrange_axes_image(self, img):\n",
        "        H,W,_ = img.shape\n",
        "        new_img = np.zeros((3,H,W))\n",
        "        for i in range(3):\n",
        "            new_img[i,:,:] = img[:,:,i]\n",
        "        return new_img\n",
        "\n",
        "    def _get_dataframe(self):\n",
        "        control_input_df = self._get_control_input_df()\n",
        "        filename_df = self._get_image_path_df()\n",
        "        df = control_input_df.merge(right=filename_df,\n",
        "                                    left_on='input_num',\n",
        "                                    right_on='index')\n",
        "        # Then, we add a column to our dataframe saying whether it's stationary or not\n",
        "        num_rows, _ = df.shape\n",
        "        df['is_stationary'] = np.zeros((num_rows), dtype=bool)\n",
        "        # Then make a copy and set is_stationary to true...\n",
        "        df_copy = df.copy()\n",
        "        df_copy['is_stationary'] = np.ones((num_rows), dtype=bool)\n",
        "        # then stack and return\n",
        "        final_df = pd.concat([df, df_copy])\n",
        "        return final_df\n",
        "\n",
        "    def _get_control_input_df(self):\n",
        "        # xcxc I'm also assuming that our columns in control_input stay static like so.\n",
        "        control_input_df = pd.read_csv(os.path.join(self.data_dir, 'control_input.txt'),\n",
        "                               names=['input_num', 'ctr1', 'ctr2'])\n",
        "        control_input_df['input_num'] = control_input_df['input_num'].astype('str')\n",
        "        return control_input_df\n",
        "    \n",
        "    def _get_image_path_df(self):\n",
        "        # A little cryptic, but it just gets the list of all filenames\n",
        "        all_files_in_out = [x[2] for x in os.walk(os.path.join(self.data_dir, '_out'))][0]\n",
        "        # Then filter out by getting only the png files. We can remove this step if need be.\n",
        "        all_files_in_out = [img_name for img_name in all_files_in_out if img_name.split('.')[1] == 'png']\n",
        "\n",
        "        # We can then make a map with our data...\n",
        "        filename_groupings = {}\n",
        "        for fn in all_files_in_out:\n",
        "            fn_number = str(int(fn.split('_')[0]))\n",
        "            if fn_number not in filename_groupings:\n",
        "                filename_groupings[fn_number] = []\n",
        "            filename_groupings[fn_number].append(fn)\n",
        "            \n",
        "        # Then make a dataframe from this dictionary\n",
        "        filename_df = pd.DataFrame.from_dict(\n",
        "            filename_groupings, orient='index').reset_index()\n",
        "        filename_df = filename_df.dropna(subset=[0,1]) # Drop if any of our images is None.\n",
        "#         filename_df = filename_df[filename_df['index'].astype('int') < 494] # Drop all the ones that are after 494\n",
        "        return filename_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHtv9Wq9nIL3",
        "colab_type": "text"
      },
      "source": [
        "### Model: `CVAE.py` (xcxc To be changed later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1p1k2hTecrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtC1hjdInHm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CVAE(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\td = 0.4\n",
        "\t\tself.z_size = 64\n",
        "\t\tself.hidden = 256\n",
        "\t\tch_sz = 3\n",
        "\t\tlast_conv = 4\n",
        "\t\tself.tensor = (1,last_conv,300,400)\n",
        "\t\tflat = np.prod(self.tensor)\n",
        "\n",
        "\t\t# channel_in, c_out, kernel_size, stride, padding\n",
        "\t\tdef convbn(ci,co,ksz,s=1,pz=0):\t\t#ReLU nonlinearity\n",
        "\t\t\treturn nn.Sequential(\n",
        "\t\t\t\tnn.Conv2d(ci,co,ksz,stride=s,padding=pz),\n",
        "\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\tnn.BatchNorm2d(co))\n",
        "\t\tdef convlast(ci,co,ksz,s=1,pz=0):\t#Sigmoid nonlinearity\n",
        "\t\t\treturn nn.Sequential(\n",
        "\t\t\t\tnn.Conv2d(ci,co,ksz,stride=s,padding=pz),\n",
        "\t\t\t\tnn.Sigmoid(),\n",
        "\t\t\t\tnn.BatchNorm2d(co))\n",
        "\t\tdef mlp(in_size,hidden):\n",
        "\t\t\treturn nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tnn.Linear(in_size,hidden),\n",
        "\t\t\t\tnn.ReLU())\n",
        "\n",
        "\t\t#Encoder NN\n",
        "\t\tself.enc = nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tconvbn(ch_sz,64,3,1,1),\n",
        "\t\t\t\tconvbn(64,16,1,1),\n",
        "\t\t\t\tconvbn(16,last_conv,1,1))\n",
        "\t\tself.m1 = mlp(flat,self.hidden)\n",
        "\t\tself.zmean = nn.Linear(self.hidden,self.z_size)\n",
        "\t\tself.zstdev = nn.Linear(self.hidden,self.z_size)\n",
        "\n",
        "\t\t#Decoder NN\n",
        "\t\tself.expand_z = nn.Linear(self.z_size,self.hidden)\n",
        "\t\tself.m2 = mlp(self.hidden,flat)\n",
        "\t\tself.dec = nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tconvbn(last_conv,16,1,1),\n",
        "\t\t\t\tconvbn(16,64,1,1),\n",
        "\t\t\t\tconvlast(64,ch_sz,1,1))\n",
        "\n",
        "\tdef encoder(self, x):\n",
        "\t\th_layer = torch.flatten(self.enc(x))\t\n",
        "\t\t# Get shapes for decoder\n",
        "\t\t# shapes1 = self.enc(x).shape\n",
        "\t\t# shapes2 = len(h_layer)\n",
        "\t\t# pdb.set_trace()\n",
        "\t\t# add control input in the following layer\n",
        "\t\th = self.m1(h_layer)\n",
        "\t\treturn h\n",
        "\n",
        "\tdef bottleneck(self, x):\n",
        "\t\tz_mean = self.zmean(x)\n",
        "\t\tz_stdev = self.zstdev(x)\n",
        "\t\t#reparam to get z latent sample\n",
        "\t\tstd = torch.exp(0.5*z_stdev)\n",
        "\t\teps = torch.randn_like(std)\n",
        "\t\tz = z_mean + eps*std\n",
        "\t\treturn z, z_mean, z_stdev\n",
        "\n",
        "\tdef decoder(self, z):\n",
        "\t\t#check the nonlinearities of this layer\n",
        "\t\th = self.expand_z(z)\n",
        "\t\th1 = self.m2(h)\n",
        "\t\t#make sure to reshape data correctly\n",
        "\t\tx = torch.reshape(h1,(self.tensor))\n",
        "\t\tout = self.dec(x)\n",
        "\t\treturn out\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\th = self.encoder(x)\n",
        "\t\tz, z_mean, z_stdev = self.bottleneck(h)\n",
        "\t\tout = self.decoder(z)\n",
        "\t\treturn out, z, z_mean, z_stdev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrXWrqmrnfEl",
        "colab_type": "text"
      },
      "source": [
        "### Training Script: `run_script.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POg-yHcNm7xU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def main():\n",
        "    model = CVAE()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999),\n",
        "                            eps=1e-08, weight_decay=0)\n",
        "    epochs = 10\n",
        "    dl = DataLoader(CarlaDataset(\"drive/My Drive/1) The C/2019-20/ESE546\"))\n",
        "    total_step = 0\n",
        "    for epoch in range(epochs):\n",
        "        for i, X in enumerate(dl):\n",
        "            left_image_t = X[0][:, 0, :, :, :] # left/right images of t\n",
        "            right_image_t = X[0][:, 1, :, :, :] # left/right images of t\n",
        "\n",
        "            # img2 = X[1] # left/right of t+1 # xcxc do the same indexing above to get l/r of t+1\n",
        "            ctrl_inputs = X[2]\n",
        "\n",
        "            left_image_t.to(device)\n",
        "            right_image_t.to(device)\n",
        "            ctrl_inputs.to(device)\n",
        "\n",
        "            left_image_t = (left_image_t/255).float()\n",
        "\n",
        "            out, z, z_mean, z_stdev = model.forward(left_image_t) # This is 3 x H x W or something idk. we also have to multiply this by 255. Then yeet this into PIL and save it.\n",
        "            loss = criterion(out, left_image_t)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            total_step += 1\n",
        "\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDF2D-rDwffz",
        "colab_type": "code",
        "outputId": "1c644d89-280b-4e5a-f008-a45098a16832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [1/1], Loss: 1.1697\n",
            "Epoch [1/10], Step [2/2], Loss: 1.1502\n",
            "Epoch [1/10], Step [3/3], Loss: 1.1493\n",
            "Epoch [1/10], Step [4/4], Loss: 1.1731\n",
            "Epoch [1/10], Step [5/5], Loss: 1.1733\n",
            "Epoch [1/10], Step [6/6], Loss: 1.1496\n",
            "Epoch [1/10], Step [7/7], Loss: 1.1501\n",
            "Epoch [1/10], Step [8/8], Loss: 1.1733\n",
            "Epoch [1/10], Step [9/9], Loss: 1.1724\n",
            "Epoch [1/10], Step [10/10], Loss: 1.1496\n",
            "Epoch [1/10], Step [11/11], Loss: 1.1740\n",
            "Epoch [1/10], Step [12/12], Loss: 1.1736\n",
            "Epoch [1/10], Step [13/13], Loss: 1.1737\n",
            "Epoch [1/10], Step [14/14], Loss: 1.1741\n",
            "Epoch [1/10], Step [15/15], Loss: 1.1495\n",
            "Epoch [1/10], Step [16/16], Loss: 1.1753\n",
            "Epoch [1/10], Step [17/17], Loss: 1.1474\n",
            "Epoch [1/10], Step [18/18], Loss: 1.1738\n",
            "Epoch [1/10], Step [19/19], Loss: 1.1704\n",
            "Epoch [1/10], Step [20/20], Loss: 1.1424\n",
            "Epoch [1/10], Step [21/21], Loss: 1.1683\n",
            "Epoch [1/10], Step [22/22], Loss: 1.1673\n",
            "Epoch [1/10], Step [23/23], Loss: 1.1339\n",
            "Epoch [1/10], Step [24/24], Loss: 1.1672\n",
            "Epoch [1/10], Step [25/25], Loss: 1.1302\n",
            "Epoch [1/10], Step [26/26], Loss: 1.1672\n",
            "Epoch [1/10], Step [27/27], Loss: 1.1344\n",
            "Epoch [1/10], Step [28/28], Loss: 1.1348\n",
            "Epoch [1/10], Step [29/29], Loss: 1.1367\n",
            "Epoch [1/10], Step [30/30], Loss: 1.1382\n",
            "Epoch [1/10], Step [31/31], Loss: 1.1584\n",
            "Epoch [1/10], Step [32/32], Loss: 1.1568\n",
            "Epoch [1/10], Step [33/33], Loss: 1.1565\n",
            "Epoch [1/10], Step [34/34], Loss: 1.1541\n",
            "Epoch [1/10], Step [35/35], Loss: 1.1529\n",
            "Epoch [1/10], Step [36/36], Loss: 1.1548\n",
            "Epoch [1/10], Step [37/37], Loss: 1.1572\n",
            "Epoch [1/10], Step [38/38], Loss: 1.1490\n",
            "Epoch [1/10], Step [39/39], Loss: 1.1410\n",
            "Epoch [1/10], Step [40/40], Loss: 1.1387\n",
            "Epoch [1/10], Step [41/41], Loss: 1.1367\n",
            "Epoch [1/10], Step [42/42], Loss: 1.1391\n",
            "Epoch [1/10], Step [43/43], Loss: 1.1320\n",
            "Epoch [1/10], Step [44/44], Loss: 1.1302\n",
            "Epoch [1/10], Step [45/45], Loss: 1.1290\n",
            "Epoch [1/10], Step [46/46], Loss: 1.1305\n",
            "Epoch [1/10], Step [47/47], Loss: 1.1468\n",
            "Epoch [1/10], Step [48/48], Loss: 1.1249\n",
            "Epoch [1/10], Step [49/49], Loss: 1.1398\n",
            "Epoch [1/10], Step [50/50], Loss: 1.1275\n",
            "Epoch [1/10], Step [51/51], Loss: 1.1281\n",
            "Epoch [1/10], Step [52/52], Loss: 1.1387\n",
            "Epoch [1/10], Step [53/53], Loss: 1.1287\n",
            "Epoch [1/10], Step [54/54], Loss: 1.1292\n",
            "Epoch [1/10], Step [55/55], Loss: 1.1317\n",
            "Epoch [1/10], Step [56/56], Loss: 1.1435\n",
            "Epoch [1/10], Step [57/57], Loss: 1.1291\n",
            "Epoch [1/10], Step [58/58], Loss: 1.1316\n",
            "Epoch [1/10], Step [59/59], Loss: 1.1330\n",
            "Epoch [1/10], Step [60/60], Loss: 1.1341\n",
            "Epoch [1/10], Step [61/61], Loss: 1.1344\n",
            "Epoch [1/10], Step [62/62], Loss: 1.1351\n",
            "Epoch [1/10], Step [63/63], Loss: 1.1580\n",
            "Epoch [1/10], Step [64/64], Loss: 1.1406\n",
            "Epoch [1/10], Step [65/65], Loss: 1.1415\n",
            "Epoch [1/10], Step [66/66], Loss: 1.1598\n",
            "Epoch [1/10], Step [67/67], Loss: 1.1631\n",
            "Epoch [1/10], Step [68/68], Loss: 1.1616\n",
            "Epoch [1/10], Step [69/69], Loss: 1.1602\n",
            "Epoch [1/10], Step [70/70], Loss: 1.1487\n",
            "Epoch [1/10], Step [71/71], Loss: 1.1640\n",
            "Epoch [1/10], Step [72/72], Loss: 1.1591\n",
            "Epoch [1/10], Step [73/73], Loss: 1.1558\n",
            "Epoch [1/10], Step [74/74], Loss: 1.1540\n",
            "Epoch [1/10], Step [75/75], Loss: 1.1466\n",
            "Epoch [1/10], Step [76/76], Loss: 1.1358\n",
            "Epoch [1/10], Step [77/77], Loss: 1.1108\n",
            "Epoch [1/10], Step [78/78], Loss: 1.1456\n",
            "Epoch [1/10], Step [79/79], Loss: 1.1058\n",
            "Epoch [1/10], Step [80/80], Loss: 1.0959\n",
            "Epoch [1/10], Step [81/81], Loss: 1.0815\n",
            "Epoch [1/10], Step [82/82], Loss: 1.1432\n",
            "Epoch [1/10], Step [83/83], Loss: 1.1453\n",
            "Epoch [1/10], Step [84/84], Loss: 1.0890\n",
            "Epoch [1/10], Step [85/85], Loss: 1.1450\n",
            "Epoch [1/10], Step [86/86], Loss: 1.1421\n",
            "Epoch [1/10], Step [87/87], Loss: 1.1101\n",
            "Epoch [1/10], Step [88/88], Loss: 1.1413\n",
            "Epoch [1/10], Step [89/89], Loss: 1.0982\n",
            "Epoch [1/10], Step [90/90], Loss: 1.1353\n",
            "Epoch [1/10], Step [91/91], Loss: 1.0816\n",
            "Epoch [1/10], Step [92/92], Loss: 1.1333\n",
            "Epoch [1/10], Step [93/93], Loss: 1.1330\n",
            "Epoch [1/10], Step [94/94], Loss: 1.0770\n",
            "Epoch [1/10], Step [95/95], Loss: 1.1338\n",
            "Epoch [1/10], Step [96/96], Loss: 1.0759\n",
            "Epoch [1/10], Step [97/97], Loss: 1.1700\n",
            "Epoch [1/10], Step [98/98], Loss: 1.1496\n",
            "Epoch [1/10], Step [99/99], Loss: 1.1508\n",
            "Epoch [1/10], Step [100/100], Loss: 1.1739\n",
            "Epoch [1/10], Step [101/101], Loss: 1.1747\n",
            "Epoch [1/10], Step [102/102], Loss: 1.1494\n",
            "Epoch [1/10], Step [103/103], Loss: 1.1492\n",
            "Epoch [1/10], Step [104/104], Loss: 1.1735\n",
            "Epoch [1/10], Step [105/105], Loss: 1.1734\n",
            "Epoch [1/10], Step [106/106], Loss: 1.1497\n",
            "Epoch [1/10], Step [107/107], Loss: 1.1735\n",
            "Epoch [1/10], Step [108/108], Loss: 1.1740\n",
            "Epoch [1/10], Step [109/109], Loss: 1.1727\n",
            "Epoch [1/10], Step [110/110], Loss: 1.1746\n",
            "Epoch [1/10], Step [111/111], Loss: 1.1490\n",
            "Epoch [1/10], Step [112/112], Loss: 1.1752\n",
            "Epoch [1/10], Step [113/113], Loss: 1.1471\n",
            "Epoch [1/10], Step [114/114], Loss: 1.1734\n",
            "Epoch [1/10], Step [115/115], Loss: 1.1697\n",
            "Epoch [1/10], Step [116/116], Loss: 1.1428\n",
            "Epoch [1/10], Step [117/117], Loss: 1.1681\n",
            "Epoch [1/10], Step [118/118], Loss: 1.1677\n",
            "Epoch [1/10], Step [119/119], Loss: 1.1337\n",
            "Epoch [1/10], Step [120/120], Loss: 1.1663\n",
            "Epoch [1/10], Step [121/121], Loss: 1.1314\n",
            "Epoch [1/10], Step [122/122], Loss: 1.1686\n",
            "Epoch [1/10], Step [123/123], Loss: 1.1344\n",
            "Epoch [1/10], Step [124/124], Loss: 1.1353\n",
            "Epoch [1/10], Step [125/125], Loss: 1.1364\n",
            "Epoch [1/10], Step [126/126], Loss: 1.1379\n",
            "Epoch [1/10], Step [127/127], Loss: 1.1588\n",
            "Epoch [1/10], Step [128/128], Loss: 1.1573\n",
            "Epoch [1/10], Step [129/129], Loss: 1.1555\n",
            "Epoch [1/10], Step [130/130], Loss: 1.1545\n",
            "Epoch [1/10], Step [131/131], Loss: 1.1542\n",
            "Epoch [1/10], Step [132/132], Loss: 1.1548\n",
            "Epoch [1/10], Step [133/133], Loss: 1.1580\n",
            "Epoch [1/10], Step [134/134], Loss: 1.1499\n",
            "Epoch [1/10], Step [135/135], Loss: 1.1411\n",
            "Epoch [1/10], Step [136/136], Loss: 1.1386\n",
            "Epoch [1/10], Step [137/137], Loss: 1.1364\n",
            "Epoch [1/10], Step [138/138], Loss: 1.1389\n",
            "Epoch [1/10], Step [139/139], Loss: 1.1317\n",
            "Epoch [1/10], Step [140/140], Loss: 1.1307\n",
            "Epoch [1/10], Step [141/141], Loss: 1.1310\n",
            "Epoch [1/10], Step [142/142], Loss: 1.1312\n",
            "Epoch [1/10], Step [143/143], Loss: 1.1471\n",
            "Epoch [1/10], Step [144/144], Loss: 1.1266\n",
            "Epoch [1/10], Step [145/145], Loss: 1.1396\n",
            "Epoch [1/10], Step [146/146], Loss: 1.1268\n",
            "Epoch [1/10], Step [147/147], Loss: 1.1283\n",
            "Epoch [1/10], Step [148/148], Loss: 1.1374\n",
            "Epoch [1/10], Step [149/149], Loss: 1.1281\n",
            "Epoch [1/10], Step [150/150], Loss: 1.1281\n",
            "Epoch [1/10], Step [151/151], Loss: 1.1313\n",
            "Epoch [1/10], Step [152/152], Loss: 1.1427\n",
            "Epoch [1/10], Step [153/153], Loss: 1.1295\n",
            "Epoch [1/10], Step [154/154], Loss: 1.1325\n",
            "Epoch [1/10], Step [155/155], Loss: 1.1333\n",
            "Epoch [1/10], Step [156/156], Loss: 1.1344\n",
            "Epoch [1/10], Step [157/157], Loss: 1.1346\n",
            "Epoch [1/10], Step [158/158], Loss: 1.1357\n",
            "Epoch [1/10], Step [159/159], Loss: 1.1596\n",
            "Epoch [1/10], Step [160/160], Loss: 1.1395\n",
            "Epoch [1/10], Step [161/161], Loss: 1.1414\n",
            "Epoch [1/10], Step [162/162], Loss: 1.1602\n",
            "Epoch [1/10], Step [163/163], Loss: 1.1640\n",
            "Epoch [1/10], Step [164/164], Loss: 1.1614\n",
            "Epoch [1/10], Step [165/165], Loss: 1.1599\n",
            "Epoch [1/10], Step [166/166], Loss: 1.1491\n",
            "Epoch [1/10], Step [167/167], Loss: 1.1628\n",
            "Epoch [1/10], Step [168/168], Loss: 1.1590\n",
            "Epoch [1/10], Step [169/169], Loss: 1.1561\n",
            "Epoch [1/10], Step [170/170], Loss: 1.1542\n",
            "Epoch [1/10], Step [171/171], Loss: 1.1467\n",
            "Epoch [1/10], Step [172/172], Loss: 1.1358\n",
            "Epoch [1/10], Step [173/173], Loss: 1.1134\n",
            "Epoch [1/10], Step [174/174], Loss: 1.1458\n",
            "Epoch [1/10], Step [175/175], Loss: 1.1057\n",
            "Epoch [1/10], Step [176/176], Loss: 1.0957\n",
            "Epoch [1/10], Step [177/177], Loss: 1.0823\n",
            "Epoch [1/10], Step [178/178], Loss: 1.1450\n",
            "Epoch [1/10], Step [179/179], Loss: 1.1437\n",
            "Epoch [1/10], Step [180/180], Loss: 1.0894\n",
            "Epoch [1/10], Step [181/181], Loss: 1.1455\n",
            "Epoch [1/10], Step [182/182], Loss: 1.1441\n",
            "Epoch [1/10], Step [183/183], Loss: 1.1089\n",
            "Epoch [1/10], Step [184/184], Loss: 1.1413\n",
            "Epoch [1/10], Step [185/185], Loss: 1.0985\n",
            "Epoch [1/10], Step [186/186], Loss: 1.1379\n",
            "Epoch [1/10], Step [187/187], Loss: 1.0816\n",
            "Epoch [1/10], Step [188/188], Loss: 1.1318\n",
            "Epoch [1/10], Step [189/189], Loss: 1.1320\n",
            "Epoch [1/10], Step [190/190], Loss: 1.0761\n",
            "Epoch [1/10], Step [191/191], Loss: 1.1336\n",
            "Epoch [1/10], Step [192/192], Loss: 1.0761\n",
            "Epoch [2/10], Step [1/193], Loss: 1.1703\n",
            "Epoch [2/10], Step [2/194], Loss: 1.1495\n",
            "Epoch [2/10], Step [3/195], Loss: 1.1498\n",
            "Epoch [2/10], Step [4/196], Loss: 1.1723\n",
            "Epoch [2/10], Step [5/197], Loss: 1.1742\n",
            "Epoch [2/10], Step [6/198], Loss: 1.1500\n",
            "Epoch [2/10], Step [7/199], Loss: 1.1491\n",
            "Epoch [2/10], Step [8/200], Loss: 1.1728\n",
            "Epoch [2/10], Step [9/201], Loss: 1.1726\n",
            "Epoch [2/10], Step [10/202], Loss: 1.1490\n",
            "Epoch [2/10], Step [11/203], Loss: 1.1725\n",
            "Epoch [2/10], Step [12/204], Loss: 1.1730\n",
            "Epoch [2/10], Step [13/205], Loss: 1.1730\n",
            "Epoch [2/10], Step [14/206], Loss: 1.1742\n",
            "Epoch [2/10], Step [15/207], Loss: 1.1495\n",
            "Epoch [2/10], Step [16/208], Loss: 1.1755\n",
            "Epoch [2/10], Step [17/209], Loss: 1.1466\n",
            "Epoch [2/10], Step [18/210], Loss: 1.1736\n",
            "Epoch [2/10], Step [19/211], Loss: 1.1705\n",
            "Epoch [2/10], Step [20/212], Loss: 1.1424\n",
            "Epoch [2/10], Step [21/213], Loss: 1.1680\n",
            "Epoch [2/10], Step [22/214], Loss: 1.1678\n",
            "Epoch [2/10], Step [23/215], Loss: 1.1339\n",
            "Epoch [2/10], Step [24/216], Loss: 1.1666\n",
            "Epoch [2/10], Step [25/217], Loss: 1.1304\n",
            "Epoch [2/10], Step [26/218], Loss: 1.1681\n",
            "Epoch [2/10], Step [27/219], Loss: 1.1333\n",
            "Epoch [2/10], Step [28/220], Loss: 1.1353\n",
            "Epoch [2/10], Step [29/221], Loss: 1.1372\n",
            "Epoch [2/10], Step [30/222], Loss: 1.1380\n",
            "Epoch [2/10], Step [31/223], Loss: 1.1583\n",
            "Epoch [2/10], Step [32/224], Loss: 1.1564\n",
            "Epoch [2/10], Step [33/225], Loss: 1.1556\n",
            "Epoch [2/10], Step [34/226], Loss: 1.1550\n",
            "Epoch [2/10], Step [35/227], Loss: 1.1535\n",
            "Epoch [2/10], Step [36/228], Loss: 1.1552\n",
            "Epoch [2/10], Step [37/229], Loss: 1.1580\n",
            "Epoch [2/10], Step [38/230], Loss: 1.1497\n",
            "Epoch [2/10], Step [39/231], Loss: 1.1422\n",
            "Epoch [2/10], Step [40/232], Loss: 1.1408\n",
            "Epoch [2/10], Step [41/233], Loss: 1.1365\n",
            "Epoch [2/10], Step [42/234], Loss: 1.1394\n",
            "Epoch [2/10], Step [43/235], Loss: 1.1325\n",
            "Epoch [2/10], Step [44/236], Loss: 1.1295\n",
            "Epoch [2/10], Step [45/237], Loss: 1.1299\n",
            "Epoch [2/10], Step [46/238], Loss: 1.1301\n",
            "Epoch [2/10], Step [47/239], Loss: 1.1471\n",
            "Epoch [2/10], Step [48/240], Loss: 1.1253\n",
            "Epoch [2/10], Step [49/241], Loss: 1.1400\n",
            "Epoch [2/10], Step [50/242], Loss: 1.1264\n",
            "Epoch [2/10], Step [51/243], Loss: 1.1281\n",
            "Epoch [2/10], Step [52/244], Loss: 1.1385\n",
            "Epoch [2/10], Step [53/245], Loss: 1.1286\n",
            "Epoch [2/10], Step [54/246], Loss: 1.1288\n",
            "Epoch [2/10], Step [55/247], Loss: 1.1324\n",
            "Epoch [2/10], Step [56/248], Loss: 1.1421\n",
            "Epoch [2/10], Step [57/249], Loss: 1.1292\n",
            "Epoch [2/10], Step [58/250], Loss: 1.1322\n",
            "Epoch [2/10], Step [59/251], Loss: 1.1335\n",
            "Epoch [2/10], Step [60/252], Loss: 1.1337\n",
            "Epoch [2/10], Step [61/253], Loss: 1.1335\n",
            "Epoch [2/10], Step [62/254], Loss: 1.1361\n",
            "Epoch [2/10], Step [63/255], Loss: 1.1580\n",
            "Epoch [2/10], Step [64/256], Loss: 1.1384\n",
            "Epoch [2/10], Step [65/257], Loss: 1.1416\n",
            "Epoch [2/10], Step [66/258], Loss: 1.1596\n",
            "Epoch [2/10], Step [67/259], Loss: 1.1630\n",
            "Epoch [2/10], Step [68/260], Loss: 1.1616\n",
            "Epoch [2/10], Step [69/261], Loss: 1.1598\n",
            "Epoch [2/10], Step [70/262], Loss: 1.1490\n",
            "Epoch [2/10], Step [71/263], Loss: 1.1640\n",
            "Epoch [2/10], Step [72/264], Loss: 1.1598\n",
            "Epoch [2/10], Step [73/265], Loss: 1.1558\n",
            "Epoch [2/10], Step [74/266], Loss: 1.1541\n",
            "Epoch [2/10], Step [75/267], Loss: 1.1462\n",
            "Epoch [2/10], Step [76/268], Loss: 1.1351\n",
            "Epoch [2/10], Step [77/269], Loss: 1.1120\n",
            "Epoch [2/10], Step [78/270], Loss: 1.1450\n",
            "Epoch [2/10], Step [79/271], Loss: 1.1054\n",
            "Epoch [2/10], Step [80/272], Loss: 1.0956\n",
            "Epoch [2/10], Step [81/273], Loss: 1.0815\n",
            "Epoch [2/10], Step [82/274], Loss: 1.1436\n",
            "Epoch [2/10], Step [83/275], Loss: 1.1444\n",
            "Epoch [2/10], Step [84/276], Loss: 1.0890\n",
            "Epoch [2/10], Step [85/277], Loss: 1.1451\n",
            "Epoch [2/10], Step [86/278], Loss: 1.1432\n",
            "Epoch [2/10], Step [87/279], Loss: 1.1104\n",
            "Epoch [2/10], Step [88/280], Loss: 1.1406\n",
            "Epoch [2/10], Step [89/281], Loss: 1.0984\n",
            "Epoch [2/10], Step [90/282], Loss: 1.1361\n",
            "Epoch [2/10], Step [91/283], Loss: 1.0819\n",
            "Epoch [2/10], Step [92/284], Loss: 1.1323\n",
            "Epoch [2/10], Step [93/285], Loss: 1.1330\n",
            "Epoch [2/10], Step [94/286], Loss: 1.0772\n",
            "Epoch [2/10], Step [95/287], Loss: 1.1333\n",
            "Epoch [2/10], Step [96/288], Loss: 1.0767\n",
            "Epoch [2/10], Step [97/289], Loss: 1.1704\n",
            "Epoch [2/10], Step [98/290], Loss: 1.1495\n",
            "Epoch [2/10], Step [99/291], Loss: 1.1498\n",
            "Epoch [2/10], Step [100/292], Loss: 1.1742\n",
            "Epoch [2/10], Step [101/293], Loss: 1.1735\n",
            "Epoch [2/10], Step [102/294], Loss: 1.1499\n",
            "Epoch [2/10], Step [103/295], Loss: 1.1499\n",
            "Epoch [2/10], Step [104/296], Loss: 1.1748\n",
            "Epoch [2/10], Step [105/297], Loss: 1.1740\n",
            "Epoch [2/10], Step [106/298], Loss: 1.1499\n",
            "Epoch [2/10], Step [107/299], Loss: 1.1726\n",
            "Epoch [2/10], Step [108/300], Loss: 1.1734\n",
            "Epoch [2/10], Step [109/301], Loss: 1.1734\n",
            "Epoch [2/10], Step [110/302], Loss: 1.1750\n",
            "Epoch [2/10], Step [111/303], Loss: 1.1497\n",
            "Epoch [2/10], Step [112/304], Loss: 1.1743\n",
            "Epoch [2/10], Step [113/305], Loss: 1.1479\n",
            "Epoch [2/10], Step [114/306], Loss: 1.1723\n",
            "Epoch [2/10], Step [115/307], Loss: 1.1705\n",
            "Epoch [2/10], Step [116/308], Loss: 1.1417\n",
            "Epoch [2/10], Step [117/309], Loss: 1.1676\n",
            "Epoch [2/10], Step [118/310], Loss: 1.1670\n",
            "Epoch [2/10], Step [119/311], Loss: 1.1336\n",
            "Epoch [2/10], Step [120/312], Loss: 1.1668\n",
            "Epoch [2/10], Step [121/313], Loss: 1.1298\n",
            "Epoch [2/10], Step [122/314], Loss: 1.1676\n",
            "Epoch [2/10], Step [123/315], Loss: 1.1350\n",
            "Epoch [2/10], Step [124/316], Loss: 1.1350\n",
            "Epoch [2/10], Step [125/317], Loss: 1.1378\n",
            "Epoch [2/10], Step [126/318], Loss: 1.1394\n",
            "Epoch [2/10], Step [127/319], Loss: 1.1591\n",
            "Epoch [2/10], Step [128/320], Loss: 1.1567\n",
            "Epoch [2/10], Step [129/321], Loss: 1.1557\n",
            "Epoch [2/10], Step [130/322], Loss: 1.1537\n",
            "Epoch [2/10], Step [131/323], Loss: 1.1532\n",
            "Epoch [2/10], Step [132/324], Loss: 1.1552\n",
            "Epoch [2/10], Step [133/325], Loss: 1.1580\n",
            "Epoch [2/10], Step [134/326], Loss: 1.1510\n",
            "Epoch [2/10], Step [135/327], Loss: 1.1410\n",
            "Epoch [2/10], Step [136/328], Loss: 1.1399\n",
            "Epoch [2/10], Step [137/329], Loss: 1.1371\n",
            "Epoch [2/10], Step [138/330], Loss: 1.1403\n",
            "Epoch [2/10], Step [139/331], Loss: 1.1311\n",
            "Epoch [2/10], Step [140/332], Loss: 1.1296\n",
            "Epoch [2/10], Step [141/333], Loss: 1.1305\n",
            "Epoch [2/10], Step [142/334], Loss: 1.1300\n",
            "Epoch [2/10], Step [143/335], Loss: 1.1464\n",
            "Epoch [2/10], Step [144/336], Loss: 1.1255\n",
            "Epoch [2/10], Step [145/337], Loss: 1.1392\n",
            "Epoch [2/10], Step [146/338], Loss: 1.1275\n",
            "Epoch [2/10], Step [147/339], Loss: 1.1278\n",
            "Epoch [2/10], Step [148/340], Loss: 1.1387\n",
            "Epoch [2/10], Step [149/341], Loss: 1.1292\n",
            "Epoch [2/10], Step [150/342], Loss: 1.1285\n",
            "Epoch [2/10], Step [151/343], Loss: 1.1307\n",
            "Epoch [2/10], Step [152/344], Loss: 1.1416\n",
            "Epoch [2/10], Step [153/345], Loss: 1.1294\n",
            "Epoch [2/10], Step [154/346], Loss: 1.1324\n",
            "Epoch [2/10], Step [155/347], Loss: 1.1335\n",
            "Epoch [2/10], Step [156/348], Loss: 1.1341\n",
            "Epoch [2/10], Step [157/349], Loss: 1.1343\n",
            "Epoch [2/10], Step [158/350], Loss: 1.1354\n",
            "Epoch [2/10], Step [159/351], Loss: 1.1593\n",
            "Epoch [2/10], Step [160/352], Loss: 1.1400\n",
            "Epoch [2/10], Step [161/353], Loss: 1.1414\n",
            "Epoch [2/10], Step [162/354], Loss: 1.1603\n",
            "Epoch [2/10], Step [163/355], Loss: 1.1635\n",
            "Epoch [2/10], Step [164/356], Loss: 1.1606\n",
            "Epoch [2/10], Step [165/357], Loss: 1.1604\n",
            "Epoch [2/10], Step [166/358], Loss: 1.1488\n",
            "Epoch [2/10], Step [167/359], Loss: 1.1625\n",
            "Epoch [2/10], Step [168/360], Loss: 1.1593\n",
            "Epoch [2/10], Step [169/361], Loss: 1.1557\n",
            "Epoch [2/10], Step [170/362], Loss: 1.1535\n",
            "Epoch [2/10], Step [171/363], Loss: 1.1463\n",
            "Epoch [2/10], Step [172/364], Loss: 1.1353\n",
            "Epoch [2/10], Step [173/365], Loss: 1.1111\n",
            "Epoch [2/10], Step [174/366], Loss: 1.1445\n",
            "Epoch [2/10], Step [175/367], Loss: 1.1067\n",
            "Epoch [2/10], Step [176/368], Loss: 1.0963\n",
            "Epoch [2/10], Step [177/369], Loss: 1.0813\n",
            "Epoch [2/10], Step [178/370], Loss: 1.1439\n",
            "Epoch [2/10], Step [179/371], Loss: 1.1448\n",
            "Epoch [2/10], Step [180/372], Loss: 1.0887\n",
            "Epoch [2/10], Step [181/373], Loss: 1.1451\n",
            "Epoch [2/10], Step [182/374], Loss: 1.1416\n",
            "Epoch [2/10], Step [183/375], Loss: 1.1099\n",
            "Epoch [2/10], Step [184/376], Loss: 1.1416\n",
            "Epoch [2/10], Step [185/377], Loss: 1.0989\n",
            "Epoch [2/10], Step [186/378], Loss: 1.1362\n",
            "Epoch [2/10], Step [187/379], Loss: 1.0826\n",
            "Epoch [2/10], Step [188/380], Loss: 1.1316\n",
            "Epoch [2/10], Step [189/381], Loss: 1.1324\n",
            "Epoch [2/10], Step [190/382], Loss: 1.0766\n",
            "Epoch [2/10], Step [191/383], Loss: 1.1322\n",
            "Epoch [2/10], Step [192/384], Loss: 1.0760\n",
            "Epoch [3/10], Step [1/385], Loss: 1.1705\n",
            "Epoch [3/10], Step [2/386], Loss: 1.1505\n",
            "Epoch [3/10], Step [3/387], Loss: 1.1492\n",
            "Epoch [3/10], Step [4/388], Loss: 1.1739\n",
            "Epoch [3/10], Step [5/389], Loss: 1.1734\n",
            "Epoch [3/10], Step [6/390], Loss: 1.1505\n",
            "Epoch [3/10], Step [7/391], Loss: 1.1501\n",
            "Epoch [3/10], Step [8/392], Loss: 1.1737\n",
            "Epoch [3/10], Step [9/393], Loss: 1.1724\n",
            "Epoch [3/10], Step [10/394], Loss: 1.1492\n",
            "Epoch [3/10], Step [11/395], Loss: 1.1734\n",
            "Epoch [3/10], Step [12/396], Loss: 1.1727\n",
            "Epoch [3/10], Step [13/397], Loss: 1.1737\n",
            "Epoch [3/10], Step [14/398], Loss: 1.1745\n",
            "Epoch [3/10], Step [15/399], Loss: 1.1501\n",
            "Epoch [3/10], Step [16/400], Loss: 1.1744\n",
            "Epoch [3/10], Step [17/401], Loss: 1.1473\n",
            "Epoch [3/10], Step [18/402], Loss: 1.1737\n",
            "Epoch [3/10], Step [19/403], Loss: 1.1699\n",
            "Epoch [3/10], Step [20/404], Loss: 1.1433\n",
            "Epoch [3/10], Step [21/405], Loss: 1.1660\n",
            "Epoch [3/10], Step [22/406], Loss: 1.1669\n",
            "Epoch [3/10], Step [23/407], Loss: 1.1333\n",
            "Epoch [3/10], Step [24/408], Loss: 1.1671\n",
            "Epoch [3/10], Step [25/409], Loss: 1.1303\n",
            "Epoch [3/10], Step [26/410], Loss: 1.1688\n",
            "Epoch [3/10], Step [27/411], Loss: 1.1330\n",
            "Epoch [3/10], Step [28/412], Loss: 1.1358\n",
            "Epoch [3/10], Step [29/413], Loss: 1.1371\n",
            "Epoch [3/10], Step [30/414], Loss: 1.1385\n",
            "Epoch [3/10], Step [31/415], Loss: 1.1589\n",
            "Epoch [3/10], Step [32/416], Loss: 1.1565\n",
            "Epoch [3/10], Step [33/417], Loss: 1.1559\n",
            "Epoch [3/10], Step [34/418], Loss: 1.1537\n",
            "Epoch [3/10], Step [35/419], Loss: 1.1529\n",
            "Epoch [3/10], Step [36/420], Loss: 1.1551\n",
            "Epoch [3/10], Step [37/421], Loss: 1.1569\n",
            "Epoch [3/10], Step [38/422], Loss: 1.1493\n",
            "Epoch [3/10], Step [39/423], Loss: 1.1425\n",
            "Epoch [3/10], Step [40/424], Loss: 1.1396\n",
            "Epoch [3/10], Step [41/425], Loss: 1.1362\n",
            "Epoch [3/10], Step [42/426], Loss: 1.1402\n",
            "Epoch [3/10], Step [43/427], Loss: 1.1326\n",
            "Epoch [3/10], Step [44/428], Loss: 1.1295\n",
            "Epoch [3/10], Step [45/429], Loss: 1.1298\n",
            "Epoch [3/10], Step [46/430], Loss: 1.1305\n",
            "Epoch [3/10], Step [47/431], Loss: 1.1466\n",
            "Epoch [3/10], Step [48/432], Loss: 1.1253\n",
            "Epoch [3/10], Step [49/433], Loss: 1.1390\n",
            "Epoch [3/10], Step [50/434], Loss: 1.1269\n",
            "Epoch [3/10], Step [51/435], Loss: 1.1277\n",
            "Epoch [3/10], Step [52/436], Loss: 1.1385\n",
            "Epoch [3/10], Step [53/437], Loss: 1.1286\n",
            "Epoch [3/10], Step [54/438], Loss: 1.1285\n",
            "Epoch [3/10], Step [55/439], Loss: 1.1312\n",
            "Epoch [3/10], Step [56/440], Loss: 1.1428\n",
            "Epoch [3/10], Step [57/441], Loss: 1.1294\n",
            "Epoch [3/10], Step [58/442], Loss: 1.1328\n",
            "Epoch [3/10], Step [59/443], Loss: 1.1326\n",
            "Epoch [3/10], Step [60/444], Loss: 1.1341\n",
            "Epoch [3/10], Step [61/445], Loss: 1.1344\n",
            "Epoch [3/10], Step [62/446], Loss: 1.1354\n",
            "Epoch [3/10], Step [63/447], Loss: 1.1586\n",
            "Epoch [3/10], Step [64/448], Loss: 1.1391\n",
            "Epoch [3/10], Step [65/449], Loss: 1.1414\n",
            "Epoch [3/10], Step [66/450], Loss: 1.1602\n",
            "Epoch [3/10], Step [67/451], Loss: 1.1627\n",
            "Epoch [3/10], Step [68/452], Loss: 1.1622\n",
            "Epoch [3/10], Step [69/453], Loss: 1.1608\n",
            "Epoch [3/10], Step [70/454], Loss: 1.1491\n",
            "Epoch [3/10], Step [71/455], Loss: 1.1630\n",
            "Epoch [3/10], Step [72/456], Loss: 1.1594\n",
            "Epoch [3/10], Step [73/457], Loss: 1.1548\n",
            "Epoch [3/10], Step [74/458], Loss: 1.1536\n",
            "Epoch [3/10], Step [75/459], Loss: 1.1466\n",
            "Epoch [3/10], Step [76/460], Loss: 1.1355\n",
            "Epoch [3/10], Step [77/461], Loss: 1.1113\n",
            "Epoch [3/10], Step [78/462], Loss: 1.1461\n",
            "Epoch [3/10], Step [79/463], Loss: 1.1061\n",
            "Epoch [3/10], Step [80/464], Loss: 1.0966\n",
            "Epoch [3/10], Step [81/465], Loss: 1.0815\n",
            "Epoch [3/10], Step [82/466], Loss: 1.1432\n",
            "Epoch [3/10], Step [83/467], Loss: 1.1445\n",
            "Epoch [3/10], Step [84/468], Loss: 1.0892\n",
            "Epoch [3/10], Step [85/469], Loss: 1.1455\n",
            "Epoch [3/10], Step [86/470], Loss: 1.1423\n",
            "Epoch [3/10], Step [87/471], Loss: 1.1096\n",
            "Epoch [3/10], Step [88/472], Loss: 1.1410\n",
            "Epoch [3/10], Step [89/473], Loss: 1.0982\n",
            "Epoch [3/10], Step [90/474], Loss: 1.1367\n",
            "Epoch [3/10], Step [91/475], Loss: 1.0821\n",
            "Epoch [3/10], Step [92/476], Loss: 1.1331\n",
            "Epoch [3/10], Step [93/477], Loss: 1.1319\n",
            "Epoch [3/10], Step [94/478], Loss: 1.0770\n",
            "Epoch [3/10], Step [95/479], Loss: 1.1342\n",
            "Epoch [3/10], Step [96/480], Loss: 1.0761\n",
            "Epoch [3/10], Step [97/481], Loss: 1.1706\n",
            "Epoch [3/10], Step [98/482], Loss: 1.1502\n",
            "Epoch [3/10], Step [99/483], Loss: 1.1505\n",
            "Epoch [3/10], Step [100/484], Loss: 1.1738\n",
            "Epoch [3/10], Step [101/485], Loss: 1.1731\n",
            "Epoch [3/10], Step [102/486], Loss: 1.1492\n",
            "Epoch [3/10], Step [103/487], Loss: 1.1489\n",
            "Epoch [3/10], Step [104/488], Loss: 1.1726\n",
            "Epoch [3/10], Step [105/489], Loss: 1.1729\n",
            "Epoch [3/10], Step [106/490], Loss: 1.1491\n",
            "Epoch [3/10], Step [107/491], Loss: 1.1726\n",
            "Epoch [3/10], Step [108/492], Loss: 1.1729\n",
            "Epoch [3/10], Step [109/493], Loss: 1.1739\n",
            "Epoch [3/10], Step [110/494], Loss: 1.1740\n",
            "Epoch [3/10], Step [111/495], Loss: 1.1487\n",
            "Epoch [3/10], Step [112/496], Loss: 1.1742\n",
            "Epoch [3/10], Step [113/497], Loss: 1.1463\n",
            "Epoch [3/10], Step [114/498], Loss: 1.1735\n",
            "Epoch [3/10], Step [115/499], Loss: 1.1707\n",
            "Epoch [3/10], Step [116/500], Loss: 1.1425\n",
            "Epoch [3/10], Step [117/501], Loss: 1.1674\n",
            "Epoch [3/10], Step [118/502], Loss: 1.1670\n",
            "Epoch [3/10], Step [119/503], Loss: 1.1346\n",
            "Epoch [3/10], Step [120/504], Loss: 1.1668\n",
            "Epoch [3/10], Step [121/505], Loss: 1.1304\n",
            "Epoch [3/10], Step [122/506], Loss: 1.1675\n",
            "Epoch [3/10], Step [123/507], Loss: 1.1345\n",
            "Epoch [3/10], Step [124/508], Loss: 1.1350\n",
            "Epoch [3/10], Step [125/509], Loss: 1.1369\n",
            "Epoch [3/10], Step [126/510], Loss: 1.1383\n",
            "Epoch [3/10], Step [127/511], Loss: 1.1587\n",
            "Epoch [3/10], Step [128/512], Loss: 1.1572\n",
            "Epoch [3/10], Step [129/513], Loss: 1.1559\n",
            "Epoch [3/10], Step [130/514], Loss: 1.1533\n",
            "Epoch [3/10], Step [131/515], Loss: 1.1527\n",
            "Epoch [3/10], Step [132/516], Loss: 1.1542\n",
            "Epoch [3/10], Step [133/517], Loss: 1.1566\n",
            "Epoch [3/10], Step [134/518], Loss: 1.1505\n",
            "Epoch [3/10], Step [135/519], Loss: 1.1413\n",
            "Epoch [3/10], Step [136/520], Loss: 1.1393\n",
            "Epoch [3/10], Step [137/521], Loss: 1.1368\n",
            "Epoch [3/10], Step [138/522], Loss: 1.1400\n",
            "Epoch [3/10], Step [139/523], Loss: 1.1312\n",
            "Epoch [3/10], Step [140/524], Loss: 1.1302\n",
            "Epoch [3/10], Step [141/525], Loss: 1.1289\n",
            "Epoch [3/10], Step [142/526], Loss: 1.1297\n",
            "Epoch [3/10], Step [143/527], Loss: 1.1473\n",
            "Epoch [3/10], Step [144/528], Loss: 1.1257\n",
            "Epoch [3/10], Step [145/529], Loss: 1.1401\n",
            "Epoch [3/10], Step [146/530], Loss: 1.1265\n",
            "Epoch [3/10], Step [147/531], Loss: 1.1277\n",
            "Epoch [3/10], Step [148/532], Loss: 1.1375\n",
            "Epoch [3/10], Step [149/533], Loss: 1.1287\n",
            "Epoch [3/10], Step [150/534], Loss: 1.1296\n",
            "Epoch [3/10], Step [151/535], Loss: 1.1308\n",
            "Epoch [3/10], Step [152/536], Loss: 1.1420\n",
            "Epoch [3/10], Step [153/537], Loss: 1.1283\n",
            "Epoch [3/10], Step [154/538], Loss: 1.1320\n",
            "Epoch [3/10], Step [155/539], Loss: 1.1333\n",
            "Epoch [3/10], Step [156/540], Loss: 1.1336\n",
            "Epoch [3/10], Step [157/541], Loss: 1.1342\n",
            "Epoch [3/10], Step [158/542], Loss: 1.1354\n",
            "Epoch [3/10], Step [159/543], Loss: 1.1587\n",
            "Epoch [3/10], Step [160/544], Loss: 1.1394\n",
            "Epoch [3/10], Step [161/545], Loss: 1.1417\n",
            "Epoch [3/10], Step [162/546], Loss: 1.1602\n",
            "Epoch [3/10], Step [163/547], Loss: 1.1632\n",
            "Epoch [3/10], Step [164/548], Loss: 1.1617\n",
            "Epoch [3/10], Step [165/549], Loss: 1.1600\n",
            "Epoch [3/10], Step [166/550], Loss: 1.1486\n",
            "Epoch [3/10], Step [167/551], Loss: 1.1629\n",
            "Epoch [3/10], Step [168/552], Loss: 1.1582\n",
            "Epoch [3/10], Step [169/553], Loss: 1.1562\n",
            "Epoch [3/10], Step [170/554], Loss: 1.1544\n",
            "Epoch [3/10], Step [171/555], Loss: 1.1459\n",
            "Epoch [3/10], Step [172/556], Loss: 1.1352\n",
            "Epoch [3/10], Step [173/557], Loss: 1.1122\n",
            "Epoch [3/10], Step [174/558], Loss: 1.1450\n",
            "Epoch [3/10], Step [175/559], Loss: 1.1057\n",
            "Epoch [3/10], Step [176/560], Loss: 1.0962\n",
            "Epoch [3/10], Step [177/561], Loss: 1.0817\n",
            "Epoch [3/10], Step [178/562], Loss: 1.1436\n",
            "Epoch [3/10], Step [179/563], Loss: 1.1442\n",
            "Epoch [3/10], Step [180/564], Loss: 1.0898\n",
            "Epoch [3/10], Step [181/565], Loss: 1.1458\n",
            "Epoch [3/10], Step [182/566], Loss: 1.1435\n",
            "Epoch [3/10], Step [183/567], Loss: 1.1106\n",
            "Epoch [3/10], Step [184/568], Loss: 1.1418\n",
            "Epoch [3/10], Step [185/569], Loss: 1.0988\n",
            "Epoch [3/10], Step [186/570], Loss: 1.1364\n",
            "Epoch [3/10], Step [187/571], Loss: 1.0822\n",
            "Epoch [3/10], Step [188/572], Loss: 1.1325\n",
            "Epoch [3/10], Step [189/573], Loss: 1.1325\n",
            "Epoch [3/10], Step [190/574], Loss: 1.0762\n",
            "Epoch [3/10], Step [191/575], Loss: 1.1324\n",
            "Epoch [3/10], Step [192/576], Loss: 1.0764\n",
            "Epoch [4/10], Step [1/577], Loss: 1.1700\n",
            "Epoch [4/10], Step [2/578], Loss: 1.1491\n",
            "Epoch [4/10], Step [3/579], Loss: 1.1498\n",
            "Epoch [4/10], Step [4/580], Loss: 1.1741\n",
            "Epoch [4/10], Step [5/581], Loss: 1.1731\n",
            "Epoch [4/10], Step [6/582], Loss: 1.1493\n",
            "Epoch [4/10], Step [7/583], Loss: 1.1492\n",
            "Epoch [4/10], Step [8/584], Loss: 1.1735\n",
            "Epoch [4/10], Step [9/585], Loss: 1.1729\n",
            "Epoch [4/10], Step [10/586], Loss: 1.1505\n",
            "Epoch [4/10], Step [11/587], Loss: 1.1742\n",
            "Epoch [4/10], Step [12/588], Loss: 1.1736\n",
            "Epoch [4/10], Step [13/589], Loss: 1.1729\n",
            "Epoch [4/10], Step [14/590], Loss: 1.1737\n",
            "Epoch [4/10], Step [15/591], Loss: 1.1495\n",
            "Epoch [4/10], Step [16/592], Loss: 1.1747\n",
            "Epoch [4/10], Step [17/593], Loss: 1.1476\n",
            "Epoch [4/10], Step [18/594], Loss: 1.1726\n",
            "Epoch [4/10], Step [19/595], Loss: 1.1708\n",
            "Epoch [4/10], Step [20/596], Loss: 1.1421\n",
            "Epoch [4/10], Step [21/597], Loss: 1.1666\n",
            "Epoch [4/10], Step [22/598], Loss: 1.1679\n",
            "Epoch [4/10], Step [23/599], Loss: 1.1339\n",
            "Epoch [4/10], Step [24/600], Loss: 1.1668\n",
            "Epoch [4/10], Step [25/601], Loss: 1.1314\n",
            "Epoch [4/10], Step [26/602], Loss: 1.1691\n",
            "Epoch [4/10], Step [27/603], Loss: 1.1334\n",
            "Epoch [4/10], Step [28/604], Loss: 1.1348\n",
            "Epoch [4/10], Step [29/605], Loss: 1.1372\n",
            "Epoch [4/10], Step [30/606], Loss: 1.1381\n",
            "Epoch [4/10], Step [31/607], Loss: 1.1578\n",
            "Epoch [4/10], Step [32/608], Loss: 1.1573\n",
            "Epoch [4/10], Step [33/609], Loss: 1.1559\n",
            "Epoch [4/10], Step [34/610], Loss: 1.1529\n",
            "Epoch [4/10], Step [35/611], Loss: 1.1524\n",
            "Epoch [4/10], Step [36/612], Loss: 1.1549\n",
            "Epoch [4/10], Step [37/613], Loss: 1.1566\n",
            "Epoch [4/10], Step [38/614], Loss: 1.1496\n",
            "Epoch [4/10], Step [39/615], Loss: 1.1422\n",
            "Epoch [4/10], Step [40/616], Loss: 1.1404\n",
            "Epoch [4/10], Step [41/617], Loss: 1.1375\n",
            "Epoch [4/10], Step [42/618], Loss: 1.1396\n",
            "Epoch [4/10], Step [43/619], Loss: 1.1319\n",
            "Epoch [4/10], Step [44/620], Loss: 1.1302\n",
            "Epoch [4/10], Step [45/621], Loss: 1.1308\n",
            "Epoch [4/10], Step [46/622], Loss: 1.1299\n",
            "Epoch [4/10], Step [47/623], Loss: 1.1478\n",
            "Epoch [4/10], Step [48/624], Loss: 1.1254\n",
            "Epoch [4/10], Step [49/625], Loss: 1.1406\n",
            "Epoch [4/10], Step [50/626], Loss: 1.1264\n",
            "Epoch [4/10], Step [51/627], Loss: 1.1270\n",
            "Epoch [4/10], Step [52/628], Loss: 1.1378\n",
            "Epoch [4/10], Step [53/629], Loss: 1.1291\n",
            "Epoch [4/10], Step [54/630], Loss: 1.1282\n",
            "Epoch [4/10], Step [55/631], Loss: 1.1310\n",
            "Epoch [4/10], Step [56/632], Loss: 1.1420\n",
            "Epoch [4/10], Step [57/633], Loss: 1.1289\n",
            "Epoch [4/10], Step [58/634], Loss: 1.1319\n",
            "Epoch [4/10], Step [59/635], Loss: 1.1329\n",
            "Epoch [4/10], Step [60/636], Loss: 1.1334\n",
            "Epoch [4/10], Step [61/637], Loss: 1.1348\n",
            "Epoch [4/10], Step [62/638], Loss: 1.1357\n",
            "Epoch [4/10], Step [63/639], Loss: 1.1592\n",
            "Epoch [4/10], Step [64/640], Loss: 1.1393\n",
            "Epoch [4/10], Step [65/641], Loss: 1.1417\n",
            "Epoch [4/10], Step [66/642], Loss: 1.1599\n",
            "Epoch [4/10], Step [67/643], Loss: 1.1637\n",
            "Epoch [4/10], Step [68/644], Loss: 1.1607\n",
            "Epoch [4/10], Step [69/645], Loss: 1.1594\n",
            "Epoch [4/10], Step [70/646], Loss: 1.1487\n",
            "Epoch [4/10], Step [71/647], Loss: 1.1634\n",
            "Epoch [4/10], Step [72/648], Loss: 1.1591\n",
            "Epoch [4/10], Step [73/649], Loss: 1.1562\n",
            "Epoch [4/10], Step [74/650], Loss: 1.1540\n",
            "Epoch [4/10], Step [75/651], Loss: 1.1466\n",
            "Epoch [4/10], Step [76/652], Loss: 1.1355\n",
            "Epoch [4/10], Step [77/653], Loss: 1.1129\n",
            "Epoch [4/10], Step [78/654], Loss: 1.1448\n",
            "Epoch [4/10], Step [79/655], Loss: 1.1053\n",
            "Epoch [4/10], Step [80/656], Loss: 1.0950\n",
            "Epoch [4/10], Step [81/657], Loss: 1.0816\n",
            "Epoch [4/10], Step [82/658], Loss: 1.1429\n",
            "Epoch [4/10], Step [83/659], Loss: 1.1448\n",
            "Epoch [4/10], Step [84/660], Loss: 1.0887\n",
            "Epoch [4/10], Step [85/661], Loss: 1.1442\n",
            "Epoch [4/10], Step [86/662], Loss: 1.1423\n",
            "Epoch [4/10], Step [87/663], Loss: 1.1107\n",
            "Epoch [4/10], Step [88/664], Loss: 1.1414\n",
            "Epoch [4/10], Step [89/665], Loss: 1.0985\n",
            "Epoch [4/10], Step [90/666], Loss: 1.1362\n",
            "Epoch [4/10], Step [91/667], Loss: 1.0824\n",
            "Epoch [4/10], Step [92/668], Loss: 1.1328\n",
            "Epoch [4/10], Step [93/669], Loss: 1.1326\n",
            "Epoch [4/10], Step [94/670], Loss: 1.0766\n",
            "Epoch [4/10], Step [95/671], Loss: 1.1329\n",
            "Epoch [4/10], Step [96/672], Loss: 1.0771\n",
            "Epoch [4/10], Step [97/673], Loss: 1.1699\n",
            "Epoch [4/10], Step [98/674], Loss: 1.1495\n",
            "Epoch [4/10], Step [99/675], Loss: 1.1473\n",
            "Epoch [4/10], Step [100/676], Loss: 1.1731\n",
            "Epoch [4/10], Step [101/677], Loss: 1.1743\n",
            "Epoch [4/10], Step [102/678], Loss: 1.1490\n",
            "Epoch [4/10], Step [103/679], Loss: 1.1496\n",
            "Epoch [4/10], Step [104/680], Loss: 1.1734\n",
            "Epoch [4/10], Step [105/681], Loss: 1.1733\n",
            "Epoch [4/10], Step [106/682], Loss: 1.1495\n",
            "Epoch [4/10], Step [107/683], Loss: 1.1735\n",
            "Epoch [4/10], Step [108/684], Loss: 1.1729\n",
            "Epoch [4/10], Step [109/685], Loss: 1.1728\n",
            "Epoch [4/10], Step [110/686], Loss: 1.1727\n",
            "Epoch [4/10], Step [111/687], Loss: 1.1491\n",
            "Epoch [4/10], Step [112/688], Loss: 1.1749\n",
            "Epoch [4/10], Step [113/689], Loss: 1.1473\n",
            "Epoch [4/10], Step [114/690], Loss: 1.1730\n",
            "Epoch [4/10], Step [115/691], Loss: 1.1703\n",
            "Epoch [4/10], Step [116/692], Loss: 1.1428\n",
            "Epoch [4/10], Step [117/693], Loss: 1.1680\n",
            "Epoch [4/10], Step [118/694], Loss: 1.1675\n",
            "Epoch [4/10], Step [119/695], Loss: 1.1338\n",
            "Epoch [4/10], Step [120/696], Loss: 1.1677\n",
            "Epoch [4/10], Step [121/697], Loss: 1.1310\n",
            "Epoch [4/10], Step [122/698], Loss: 1.1691\n",
            "Epoch [4/10], Step [123/699], Loss: 1.1349\n",
            "Epoch [4/10], Step [124/700], Loss: 1.1352\n",
            "Epoch [4/10], Step [125/701], Loss: 1.1369\n",
            "Epoch [4/10], Step [126/702], Loss: 1.1385\n",
            "Epoch [4/10], Step [127/703], Loss: 1.1588\n",
            "Epoch [4/10], Step [128/704], Loss: 1.1574\n",
            "Epoch [4/10], Step [129/705], Loss: 1.1560\n",
            "Epoch [4/10], Step [130/706], Loss: 1.1535\n",
            "Epoch [4/10], Step [131/707], Loss: 1.1526\n",
            "Epoch [4/10], Step [132/708], Loss: 1.1548\n",
            "Epoch [4/10], Step [133/709], Loss: 1.1573\n",
            "Epoch [4/10], Step [134/710], Loss: 1.1499\n",
            "Epoch [4/10], Step [135/711], Loss: 1.1423\n",
            "Epoch [4/10], Step [136/712], Loss: 1.1400\n",
            "Epoch [4/10], Step [137/713], Loss: 1.1366\n",
            "Epoch [4/10], Step [138/714], Loss: 1.1394\n",
            "Epoch [4/10], Step [139/715], Loss: 1.1316\n",
            "Epoch [4/10], Step [140/716], Loss: 1.1301\n",
            "Epoch [4/10], Step [141/717], Loss: 1.1298\n",
            "Epoch [4/10], Step [142/718], Loss: 1.1301\n",
            "Epoch [4/10], Step [143/719], Loss: 1.1470\n",
            "Epoch [4/10], Step [144/720], Loss: 1.1258\n",
            "Epoch [4/10], Step [145/721], Loss: 1.1401\n",
            "Epoch [4/10], Step [146/722], Loss: 1.1257\n",
            "Epoch [4/10], Step [147/723], Loss: 1.1274\n",
            "Epoch [4/10], Step [148/724], Loss: 1.1370\n",
            "Epoch [4/10], Step [149/725], Loss: 1.1290\n",
            "Epoch [4/10], Step [150/726], Loss: 1.1281\n",
            "Epoch [4/10], Step [151/727], Loss: 1.1308\n",
            "Epoch [4/10], Step [152/728], Loss: 1.1425\n",
            "Epoch [4/10], Step [153/729], Loss: 1.1293\n",
            "Epoch [4/10], Step [154/730], Loss: 1.1312\n",
            "Epoch [4/10], Step [155/731], Loss: 1.1331\n",
            "Epoch [4/10], Step [156/732], Loss: 1.1335\n",
            "Epoch [4/10], Step [157/733], Loss: 1.1331\n",
            "Epoch [4/10], Step [158/734], Loss: 1.1358\n",
            "Epoch [4/10], Step [159/735], Loss: 1.1587\n",
            "Epoch [4/10], Step [160/736], Loss: 1.1392\n",
            "Epoch [4/10], Step [161/737], Loss: 1.1412\n",
            "Epoch [4/10], Step [162/738], Loss: 1.1592\n",
            "Epoch [4/10], Step [163/739], Loss: 1.1635\n",
            "Epoch [4/10], Step [164/740], Loss: 1.1612\n",
            "Epoch [4/10], Step [165/741], Loss: 1.1607\n",
            "Epoch [4/10], Step [166/742], Loss: 1.1499\n",
            "Epoch [4/10], Step [167/743], Loss: 1.1634\n",
            "Epoch [4/10], Step [168/744], Loss: 1.1592\n",
            "Epoch [4/10], Step [169/745], Loss: 1.1555\n",
            "Epoch [4/10], Step [170/746], Loss: 1.1537\n",
            "Epoch [4/10], Step [171/747], Loss: 1.1461\n",
            "Epoch [4/10], Step [172/748], Loss: 1.1343\n",
            "Epoch [4/10], Step [173/749], Loss: 1.1122\n",
            "Epoch [4/10], Step [174/750], Loss: 1.1454\n",
            "Epoch [4/10], Step [175/751], Loss: 1.1052\n",
            "Epoch [4/10], Step [176/752], Loss: 1.0954\n",
            "Epoch [4/10], Step [177/753], Loss: 1.0816\n",
            "Epoch [4/10], Step [178/754], Loss: 1.1438\n",
            "Epoch [4/10], Step [179/755], Loss: 1.1438\n",
            "Epoch [4/10], Step [180/756], Loss: 1.0890\n",
            "Epoch [4/10], Step [181/757], Loss: 1.1458\n",
            "Epoch [4/10], Step [182/758], Loss: 1.1433\n",
            "Epoch [4/10], Step [183/759], Loss: 1.1106\n",
            "Epoch [4/10], Step [184/760], Loss: 1.1414\n",
            "Epoch [4/10], Step [185/761], Loss: 1.0985\n",
            "Epoch [4/10], Step [186/762], Loss: 1.1366\n",
            "Epoch [4/10], Step [187/763], Loss: 1.0820\n",
            "Epoch [4/10], Step [188/764], Loss: 1.1329\n",
            "Epoch [4/10], Step [189/765], Loss: 1.1326\n",
            "Epoch [4/10], Step [190/766], Loss: 1.0766\n",
            "Epoch [4/10], Step [191/767], Loss: 1.1334\n",
            "Epoch [4/10], Step [192/768], Loss: 1.0767\n",
            "Epoch [5/10], Step [1/769], Loss: 1.1702\n",
            "Epoch [5/10], Step [2/770], Loss: 1.1492\n",
            "Epoch [5/10], Step [3/771], Loss: 1.1499\n",
            "Epoch [5/10], Step [4/772], Loss: 1.1736\n",
            "Epoch [5/10], Step [5/773], Loss: 1.1738\n",
            "Epoch [5/10], Step [6/774], Loss: 1.1487\n",
            "Epoch [5/10], Step [7/775], Loss: 1.1505\n",
            "Epoch [5/10], Step [8/776], Loss: 1.1742\n",
            "Epoch [5/10], Step [9/777], Loss: 1.1740\n",
            "Epoch [5/10], Step [10/778], Loss: 1.1500\n",
            "Epoch [5/10], Step [11/779], Loss: 1.1738\n",
            "Epoch [5/10], Step [12/780], Loss: 1.1740\n",
            "Epoch [5/10], Step [13/781], Loss: 1.1737\n",
            "Epoch [5/10], Step [14/782], Loss: 1.1752\n",
            "Epoch [5/10], Step [15/783], Loss: 1.1493\n",
            "Epoch [5/10], Step [16/784], Loss: 1.1743\n",
            "Epoch [5/10], Step [17/785], Loss: 1.1476\n",
            "Epoch [5/10], Step [18/786], Loss: 1.1738\n",
            "Epoch [5/10], Step [19/787], Loss: 1.1695\n",
            "Epoch [5/10], Step [20/788], Loss: 1.1418\n",
            "Epoch [5/10], Step [21/789], Loss: 1.1675\n",
            "Epoch [5/10], Step [22/790], Loss: 1.1678\n",
            "Epoch [5/10], Step [23/791], Loss: 1.1343\n",
            "Epoch [5/10], Step [24/792], Loss: 1.1674\n",
            "Epoch [5/10], Step [25/793], Loss: 1.1317\n",
            "Epoch [5/10], Step [26/794], Loss: 1.1686\n",
            "Epoch [5/10], Step [27/795], Loss: 1.1344\n",
            "Epoch [5/10], Step [28/796], Loss: 1.1347\n",
            "Epoch [5/10], Step [29/797], Loss: 1.1378\n",
            "Epoch [5/10], Step [30/798], Loss: 1.1380\n",
            "Epoch [5/10], Step [31/799], Loss: 1.1582\n",
            "Epoch [5/10], Step [32/800], Loss: 1.1574\n",
            "Epoch [5/10], Step [33/801], Loss: 1.1563\n",
            "Epoch [5/10], Step [34/802], Loss: 1.1541\n",
            "Epoch [5/10], Step [35/803], Loss: 1.1526\n",
            "Epoch [5/10], Step [36/804], Loss: 1.1550\n",
            "Epoch [5/10], Step [37/805], Loss: 1.1575\n",
            "Epoch [5/10], Step [38/806], Loss: 1.1498\n",
            "Epoch [5/10], Step [39/807], Loss: 1.1418\n",
            "Epoch [5/10], Step [40/808], Loss: 1.1394\n",
            "Epoch [5/10], Step [41/809], Loss: 1.1378\n",
            "Epoch [5/10], Step [42/810], Loss: 1.1397\n",
            "Epoch [5/10], Step [43/811], Loss: 1.1317\n",
            "Epoch [5/10], Step [44/812], Loss: 1.1298\n",
            "Epoch [5/10], Step [45/813], Loss: 1.1303\n",
            "Epoch [5/10], Step [46/814], Loss: 1.1312\n",
            "Epoch [5/10], Step [47/815], Loss: 1.1467\n",
            "Epoch [5/10], Step [48/816], Loss: 1.1255\n",
            "Epoch [5/10], Step [49/817], Loss: 1.1398\n",
            "Epoch [5/10], Step [50/818], Loss: 1.1269\n",
            "Epoch [5/10], Step [51/819], Loss: 1.1278\n",
            "Epoch [5/10], Step [52/820], Loss: 1.1381\n",
            "Epoch [5/10], Step [53/821], Loss: 1.1292\n",
            "Epoch [5/10], Step [54/822], Loss: 1.1285\n",
            "Epoch [5/10], Step [55/823], Loss: 1.1313\n",
            "Epoch [5/10], Step [56/824], Loss: 1.1429\n",
            "Epoch [5/10], Step [57/825], Loss: 1.1292\n",
            "Epoch [5/10], Step [58/826], Loss: 1.1327\n",
            "Epoch [5/10], Step [59/827], Loss: 1.1331\n",
            "Epoch [5/10], Step [60/828], Loss: 1.1331\n",
            "Epoch [5/10], Step [61/829], Loss: 1.1330\n",
            "Epoch [5/10], Step [62/830], Loss: 1.1352\n",
            "Epoch [5/10], Step [63/831], Loss: 1.1581\n",
            "Epoch [5/10], Step [64/832], Loss: 1.1396\n",
            "Epoch [5/10], Step [65/833], Loss: 1.1419\n",
            "Epoch [5/10], Step [66/834], Loss: 1.1608\n",
            "Epoch [5/10], Step [67/835], Loss: 1.1630\n",
            "Epoch [5/10], Step [68/836], Loss: 1.1615\n",
            "Epoch [5/10], Step [69/837], Loss: 1.1591\n",
            "Epoch [5/10], Step [70/838], Loss: 1.1491\n",
            "Epoch [5/10], Step [71/839], Loss: 1.1625\n",
            "Epoch [5/10], Step [72/840], Loss: 1.1595\n",
            "Epoch [5/10], Step [73/841], Loss: 1.1555\n",
            "Epoch [5/10], Step [74/842], Loss: 1.1540\n",
            "Epoch [5/10], Step [75/843], Loss: 1.1460\n",
            "Epoch [5/10], Step [76/844], Loss: 1.1359\n",
            "Epoch [5/10], Step [77/845], Loss: 1.1117\n",
            "Epoch [5/10], Step [78/846], Loss: 1.1458\n",
            "Epoch [5/10], Step [79/847], Loss: 1.1050\n",
            "Epoch [5/10], Step [80/848], Loss: 1.0968\n",
            "Epoch [5/10], Step [81/849], Loss: 1.0822\n",
            "Epoch [5/10], Step [82/850], Loss: 1.1447\n",
            "Epoch [5/10], Step [83/851], Loss: 1.1443\n",
            "Epoch [5/10], Step [84/852], Loss: 1.0899\n",
            "Epoch [5/10], Step [85/853], Loss: 1.1448\n",
            "Epoch [5/10], Step [86/854], Loss: 1.1430\n",
            "Epoch [5/10], Step [87/855], Loss: 1.1105\n",
            "Epoch [5/10], Step [88/856], Loss: 1.1417\n",
            "Epoch [5/10], Step [89/857], Loss: 1.0988\n",
            "Epoch [5/10], Step [90/858], Loss: 1.1362\n",
            "Epoch [5/10], Step [91/859], Loss: 1.0823\n",
            "Epoch [5/10], Step [92/860], Loss: 1.1324\n",
            "Epoch [5/10], Step [93/861], Loss: 1.1331\n",
            "Epoch [5/10], Step [94/862], Loss: 1.0763\n",
            "Epoch [5/10], Step [95/863], Loss: 1.1327\n",
            "Epoch [5/10], Step [96/864], Loss: 1.0768\n",
            "Epoch [5/10], Step [97/865], Loss: 1.1695\n",
            "Epoch [5/10], Step [98/866], Loss: 1.1508\n",
            "Epoch [5/10], Step [99/867], Loss: 1.1495\n",
            "Epoch [5/10], Step [100/868], Loss: 1.1739\n",
            "Epoch [5/10], Step [101/869], Loss: 1.1737\n",
            "Epoch [5/10], Step [102/870], Loss: 1.1502\n",
            "Epoch [5/10], Step [103/871], Loss: 1.1492\n",
            "Epoch [5/10], Step [104/872], Loss: 1.1739\n",
            "Epoch [5/10], Step [105/873], Loss: 1.1744\n",
            "Epoch [5/10], Step [106/874], Loss: 1.1490\n",
            "Epoch [5/10], Step [107/875], Loss: 1.1729\n",
            "Epoch [5/10], Step [108/876], Loss: 1.1736\n",
            "Epoch [5/10], Step [109/877], Loss: 1.1739\n",
            "Epoch [5/10], Step [110/878], Loss: 1.1743\n",
            "Epoch [5/10], Step [111/879], Loss: 1.1498\n",
            "Epoch [5/10], Step [112/880], Loss: 1.1740\n",
            "Epoch [5/10], Step [113/881], Loss: 1.1476\n",
            "Epoch [5/10], Step [114/882], Loss: 1.1727\n",
            "Epoch [5/10], Step [115/883], Loss: 1.1710\n",
            "Epoch [5/10], Step [116/884], Loss: 1.1425\n",
            "Epoch [5/10], Step [117/885], Loss: 1.1679\n",
            "Epoch [5/10], Step [118/886], Loss: 1.1678\n",
            "Epoch [5/10], Step [119/887], Loss: 1.1327\n",
            "Epoch [5/10], Step [120/888], Loss: 1.1667\n",
            "Epoch [5/10], Step [121/889], Loss: 1.1312\n",
            "Epoch [5/10], Step [122/890], Loss: 1.1680\n",
            "Epoch [5/10], Step [123/891], Loss: 1.1342\n",
            "Epoch [5/10], Step [124/892], Loss: 1.1355\n",
            "Epoch [5/10], Step [125/893], Loss: 1.1376\n",
            "Epoch [5/10], Step [126/894], Loss: 1.1376\n",
            "Epoch [5/10], Step [127/895], Loss: 1.1583\n",
            "Epoch [5/10], Step [128/896], Loss: 1.1572\n",
            "Epoch [5/10], Step [129/897], Loss: 1.1572\n",
            "Epoch [5/10], Step [130/898], Loss: 1.1539\n",
            "Epoch [5/10], Step [131/899], Loss: 1.1521\n",
            "Epoch [5/10], Step [132/900], Loss: 1.1540\n",
            "Epoch [5/10], Step [133/901], Loss: 1.1571\n",
            "Epoch [5/10], Step [134/902], Loss: 1.1493\n",
            "Epoch [5/10], Step [135/903], Loss: 1.1428\n",
            "Epoch [5/10], Step [136/904], Loss: 1.1387\n",
            "Epoch [5/10], Step [137/905], Loss: 1.1366\n",
            "Epoch [5/10], Step [138/906], Loss: 1.1403\n",
            "Epoch [5/10], Step [139/907], Loss: 1.1317\n",
            "Epoch [5/10], Step [140/908], Loss: 1.1307\n",
            "Epoch [5/10], Step [141/909], Loss: 1.1299\n",
            "Epoch [5/10], Step [142/910], Loss: 1.1296\n",
            "Epoch [5/10], Step [143/911], Loss: 1.1473\n",
            "Epoch [5/10], Step [144/912], Loss: 1.1257\n",
            "Epoch [5/10], Step [145/913], Loss: 1.1398\n",
            "Epoch [5/10], Step [146/914], Loss: 1.1264\n",
            "Epoch [5/10], Step [147/915], Loss: 1.1272\n",
            "Epoch [5/10], Step [148/916], Loss: 1.1365\n",
            "Epoch [5/10], Step [149/917], Loss: 1.1298\n",
            "Epoch [5/10], Step [150/918], Loss: 1.1292\n",
            "Epoch [5/10], Step [151/919], Loss: 1.1319\n",
            "Epoch [5/10], Step [152/920], Loss: 1.1413\n",
            "Epoch [5/10], Step [153/921], Loss: 1.1289\n",
            "Epoch [5/10], Step [154/922], Loss: 1.1305\n",
            "Epoch [5/10], Step [155/923], Loss: 1.1327\n",
            "Epoch [5/10], Step [156/924], Loss: 1.1336\n",
            "Epoch [5/10], Step [157/925], Loss: 1.1336\n",
            "Epoch [5/10], Step [158/926], Loss: 1.1350\n",
            "Epoch [5/10], Step [159/927], Loss: 1.1581\n",
            "Epoch [5/10], Step [160/928], Loss: 1.1387\n",
            "Epoch [5/10], Step [161/929], Loss: 1.1418\n",
            "Epoch [5/10], Step [162/930], Loss: 1.1599\n",
            "Epoch [5/10], Step [163/931], Loss: 1.1636\n",
            "Epoch [5/10], Step [164/932], Loss: 1.1615\n",
            "Epoch [5/10], Step [165/933], Loss: 1.1592\n",
            "Epoch [5/10], Step [166/934], Loss: 1.1491\n",
            "Epoch [5/10], Step [167/935], Loss: 1.1633\n",
            "Epoch [5/10], Step [168/936], Loss: 1.1600\n",
            "Epoch [5/10], Step [169/937], Loss: 1.1556\n",
            "Epoch [5/10], Step [170/938], Loss: 1.1532\n",
            "Epoch [5/10], Step [171/939], Loss: 1.1471\n",
            "Epoch [5/10], Step [172/940], Loss: 1.1356\n",
            "Epoch [5/10], Step [173/941], Loss: 1.1123\n",
            "Epoch [5/10], Step [174/942], Loss: 1.1455\n",
            "Epoch [5/10], Step [175/943], Loss: 1.1059\n",
            "Epoch [5/10], Step [176/944], Loss: 1.0952\n",
            "Epoch [5/10], Step [177/945], Loss: 1.0816\n",
            "Epoch [5/10], Step [178/946], Loss: 1.1444\n",
            "Epoch [5/10], Step [179/947], Loss: 1.1442\n",
            "Epoch [5/10], Step [180/948], Loss: 1.0895\n",
            "Epoch [5/10], Step [181/949], Loss: 1.1453\n",
            "Epoch [5/10], Step [182/950], Loss: 1.1413\n",
            "Epoch [5/10], Step [183/951], Loss: 1.1098\n",
            "Epoch [5/10], Step [184/952], Loss: 1.1419\n",
            "Epoch [5/10], Step [185/953], Loss: 1.0986\n",
            "Epoch [5/10], Step [186/954], Loss: 1.1369\n",
            "Epoch [5/10], Step [187/955], Loss: 1.0818\n",
            "Epoch [5/10], Step [188/956], Loss: 1.1328\n",
            "Epoch [5/10], Step [189/957], Loss: 1.1323\n",
            "Epoch [5/10], Step [190/958], Loss: 1.0767\n",
            "Epoch [5/10], Step [191/959], Loss: 1.1328\n",
            "Epoch [5/10], Step [192/960], Loss: 1.0761\n",
            "Epoch [6/10], Step [1/961], Loss: 1.1695\n",
            "Epoch [6/10], Step [2/962], Loss: 1.1496\n",
            "Epoch [6/10], Step [3/963], Loss: 1.1498\n",
            "Epoch [6/10], Step [4/964], Loss: 1.1739\n",
            "Epoch [6/10], Step [5/965], Loss: 1.1730\n",
            "Epoch [6/10], Step [6/966], Loss: 1.1505\n",
            "Epoch [6/10], Step [7/967], Loss: 1.1499\n",
            "Epoch [6/10], Step [8/968], Loss: 1.1740\n",
            "Epoch [6/10], Step [9/969], Loss: 1.1726\n",
            "Epoch [6/10], Step [10/970], Loss: 1.1495\n",
            "Epoch [6/10], Step [11/971], Loss: 1.1739\n",
            "Epoch [6/10], Step [12/972], Loss: 1.1732\n",
            "Epoch [6/10], Step [13/973], Loss: 1.1739\n",
            "Epoch [6/10], Step [14/974], Loss: 1.1730\n",
            "Epoch [6/10], Step [15/975], Loss: 1.1498\n",
            "Epoch [6/10], Step [16/976], Loss: 1.1753\n",
            "Epoch [6/10], Step [17/977], Loss: 1.1476\n",
            "Epoch [6/10], Step [18/978], Loss: 1.1732\n",
            "Epoch [6/10], Step [19/979], Loss: 1.1703\n",
            "Epoch [6/10], Step [20/980], Loss: 1.1439\n",
            "Epoch [6/10], Step [21/981], Loss: 1.1670\n",
            "Epoch [6/10], Step [22/982], Loss: 1.1667\n",
            "Epoch [6/10], Step [23/983], Loss: 1.1334\n",
            "Epoch [6/10], Step [24/984], Loss: 1.1665\n",
            "Epoch [6/10], Step [25/985], Loss: 1.1304\n",
            "Epoch [6/10], Step [26/986], Loss: 1.1676\n",
            "Epoch [6/10], Step [27/987], Loss: 1.1344\n",
            "Epoch [6/10], Step [28/988], Loss: 1.1352\n",
            "Epoch [6/10], Step [29/989], Loss: 1.1377\n",
            "Epoch [6/10], Step [30/990], Loss: 1.1390\n",
            "Epoch [6/10], Step [31/991], Loss: 1.1580\n",
            "Epoch [6/10], Step [32/992], Loss: 1.1580\n",
            "Epoch [6/10], Step [33/993], Loss: 1.1559\n",
            "Epoch [6/10], Step [34/994], Loss: 1.1529\n",
            "Epoch [6/10], Step [35/995], Loss: 1.1532\n",
            "Epoch [6/10], Step [36/996], Loss: 1.1551\n",
            "Epoch [6/10], Step [37/997], Loss: 1.1583\n",
            "Epoch [6/10], Step [38/998], Loss: 1.1499\n",
            "Epoch [6/10], Step [39/999], Loss: 1.1419\n",
            "Epoch [6/10], Step [40/1000], Loss: 1.1400\n",
            "Epoch [6/10], Step [41/1001], Loss: 1.1375\n",
            "Epoch [6/10], Step [42/1002], Loss: 1.1400\n",
            "Epoch [6/10], Step [43/1003], Loss: 1.1318\n",
            "Epoch [6/10], Step [44/1004], Loss: 1.1304\n",
            "Epoch [6/10], Step [45/1005], Loss: 1.1295\n",
            "Epoch [6/10], Step [46/1006], Loss: 1.1304\n",
            "Epoch [6/10], Step [47/1007], Loss: 1.1466\n",
            "Epoch [6/10], Step [48/1008], Loss: 1.1258\n",
            "Epoch [6/10], Step [49/1009], Loss: 1.1411\n",
            "Epoch [6/10], Step [50/1010], Loss: 1.1270\n",
            "Epoch [6/10], Step [51/1011], Loss: 1.1272\n",
            "Epoch [6/10], Step [52/1012], Loss: 1.1378\n",
            "Epoch [6/10], Step [53/1013], Loss: 1.1296\n",
            "Epoch [6/10], Step [54/1014], Loss: 1.1287\n",
            "Epoch [6/10], Step [55/1015], Loss: 1.1316\n",
            "Epoch [6/10], Step [56/1016], Loss: 1.1423\n",
            "Epoch [6/10], Step [57/1017], Loss: 1.1300\n",
            "Epoch [6/10], Step [58/1018], Loss: 1.1316\n",
            "Epoch [6/10], Step [59/1019], Loss: 1.1331\n",
            "Epoch [6/10], Step [60/1020], Loss: 1.1333\n",
            "Epoch [6/10], Step [61/1021], Loss: 1.1346\n",
            "Epoch [6/10], Step [62/1022], Loss: 1.1358\n",
            "Epoch [6/10], Step [63/1023], Loss: 1.1586\n",
            "Epoch [6/10], Step [64/1024], Loss: 1.1397\n",
            "Epoch [6/10], Step [65/1025], Loss: 1.1417\n",
            "Epoch [6/10], Step [66/1026], Loss: 1.1598\n",
            "Epoch [6/10], Step [67/1027], Loss: 1.1625\n",
            "Epoch [6/10], Step [68/1028], Loss: 1.1608\n",
            "Epoch [6/10], Step [69/1029], Loss: 1.1606\n",
            "Epoch [6/10], Step [70/1030], Loss: 1.1494\n",
            "Epoch [6/10], Step [71/1031], Loss: 1.1636\n",
            "Epoch [6/10], Step [72/1032], Loss: 1.1592\n",
            "Epoch [6/10], Step [73/1033], Loss: 1.1559\n",
            "Epoch [6/10], Step [74/1034], Loss: 1.1539\n",
            "Epoch [6/10], Step [75/1035], Loss: 1.1459\n",
            "Epoch [6/10], Step [76/1036], Loss: 1.1344\n",
            "Epoch [6/10], Step [77/1037], Loss: 1.1123\n",
            "Epoch [6/10], Step [78/1038], Loss: 1.1449\n",
            "Epoch [6/10], Step [79/1039], Loss: 1.1062\n",
            "Epoch [6/10], Step [80/1040], Loss: 1.0960\n",
            "Epoch [6/10], Step [81/1041], Loss: 1.0821\n",
            "Epoch [6/10], Step [82/1042], Loss: 1.1438\n",
            "Epoch [6/10], Step [83/1043], Loss: 1.1441\n",
            "Epoch [6/10], Step [84/1044], Loss: 1.0892\n",
            "Epoch [6/10], Step [85/1045], Loss: 1.1450\n",
            "Epoch [6/10], Step [86/1046], Loss: 1.1434\n",
            "Epoch [6/10], Step [87/1047], Loss: 1.1106\n",
            "Epoch [6/10], Step [88/1048], Loss: 1.1420\n",
            "Epoch [6/10], Step [89/1049], Loss: 1.0992\n",
            "Epoch [6/10], Step [90/1050], Loss: 1.1360\n",
            "Epoch [6/10], Step [91/1051], Loss: 1.0815\n",
            "Epoch [6/10], Step [92/1052], Loss: 1.1331\n",
            "Epoch [6/10], Step [93/1053], Loss: 1.1322\n",
            "Epoch [6/10], Step [94/1054], Loss: 1.0756\n",
            "Epoch [6/10], Step [95/1055], Loss: 1.1331\n",
            "Epoch [6/10], Step [96/1056], Loss: 1.0762\n",
            "Epoch [6/10], Step [97/1057], Loss: 1.1706\n",
            "Epoch [6/10], Step [98/1058], Loss: 1.1502\n",
            "Epoch [6/10], Step [99/1059], Loss: 1.1505\n",
            "Epoch [6/10], Step [100/1060], Loss: 1.1744\n",
            "Epoch [6/10], Step [101/1061], Loss: 1.1741\n",
            "Epoch [6/10], Step [102/1062], Loss: 1.1496\n",
            "Epoch [6/10], Step [103/1063], Loss: 1.1496\n",
            "Epoch [6/10], Step [104/1064], Loss: 1.1740\n",
            "Epoch [6/10], Step [105/1065], Loss: 1.1722\n",
            "Epoch [6/10], Step [106/1066], Loss: 1.1498\n",
            "Epoch [6/10], Step [107/1067], Loss: 1.1730\n",
            "Epoch [6/10], Step [108/1068], Loss: 1.1729\n",
            "Epoch [6/10], Step [109/1069], Loss: 1.1738\n",
            "Epoch [6/10], Step [110/1070], Loss: 1.1734\n",
            "Epoch [6/10], Step [111/1071], Loss: 1.1502\n",
            "Epoch [6/10], Step [112/1072], Loss: 1.1751\n",
            "Epoch [6/10], Step [113/1073], Loss: 1.1455\n",
            "Epoch [6/10], Step [114/1074], Loss: 1.1732\n",
            "Epoch [6/10], Step [115/1075], Loss: 1.1706\n",
            "Epoch [6/10], Step [116/1076], Loss: 1.1434\n",
            "Epoch [6/10], Step [117/1077], Loss: 1.1679\n",
            "Epoch [6/10], Step [118/1078], Loss: 1.1671\n",
            "Epoch [6/10], Step [119/1079], Loss: 1.1344\n",
            "Epoch [6/10], Step [120/1080], Loss: 1.1668\n",
            "Epoch [6/10], Step [121/1081], Loss: 1.1305\n",
            "Epoch [6/10], Step [122/1082], Loss: 1.1679\n",
            "Epoch [6/10], Step [123/1083], Loss: 1.1341\n",
            "Epoch [6/10], Step [124/1084], Loss: 1.1345\n",
            "Epoch [6/10], Step [125/1085], Loss: 1.1367\n",
            "Epoch [6/10], Step [126/1086], Loss: 1.1387\n",
            "Epoch [6/10], Step [127/1087], Loss: 1.1581\n",
            "Epoch [6/10], Step [128/1088], Loss: 1.1568\n",
            "Epoch [6/10], Step [129/1089], Loss: 1.1555\n",
            "Epoch [6/10], Step [130/1090], Loss: 1.1546\n",
            "Epoch [6/10], Step [131/1091], Loss: 1.1533\n",
            "Epoch [6/10], Step [132/1092], Loss: 1.1553\n",
            "Epoch [6/10], Step [133/1093], Loss: 1.1571\n",
            "Epoch [6/10], Step [134/1094], Loss: 1.1502\n",
            "Epoch [6/10], Step [135/1095], Loss: 1.1425\n",
            "Epoch [6/10], Step [136/1096], Loss: 1.1402\n",
            "Epoch [6/10], Step [137/1097], Loss: 1.1372\n",
            "Epoch [6/10], Step [138/1098], Loss: 1.1413\n",
            "Epoch [6/10], Step [139/1099], Loss: 1.1317\n",
            "Epoch [6/10], Step [140/1100], Loss: 1.1303\n",
            "Epoch [6/10], Step [141/1101], Loss: 1.1299\n",
            "Epoch [6/10], Step [142/1102], Loss: 1.1306\n",
            "Epoch [6/10], Step [143/1103], Loss: 1.1479\n",
            "Epoch [6/10], Step [144/1104], Loss: 1.1264\n",
            "Epoch [6/10], Step [145/1105], Loss: 1.1397\n",
            "Epoch [6/10], Step [146/1106], Loss: 1.1258\n",
            "Epoch [6/10], Step [147/1107], Loss: 1.1275\n",
            "Epoch [6/10], Step [148/1108], Loss: 1.1384\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-c3be6bd9f614>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_image_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}