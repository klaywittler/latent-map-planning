{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "velocityModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "widjDd5ekWdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shared by all\n",
        "import os, pickle\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils # We should use this eventually.\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "\n",
        "# For DataLoader\n",
        "from PIL import Image\n",
        "import numbers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HDddlmbkgjz",
        "colab_type": "code",
        "outputId": "3d55a780-7b18-401b-997b-3eaa59d38525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/' )#, force_remount=True)\n",
        "\n",
        "base = '/content/drive/My Drive/School/Fall 2019/ESE 546/project/'\n",
        "os.makedirs(base+'checkpoints', exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvl8kypVkseG",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Code: `VelocityPredicitionCaralDataset.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4JBoptCkmWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils # We should use this eventually.\n",
        "from PIL import Image\n",
        "import numbers\n",
        "import glob\n",
        "\n",
        "class VelocityPredictionCarlaDataSet(Dataset):\n",
        "    def __init__(self, data_dir, goal_images={}, delta=100, load_as_grayscale=False, transform=None):\n",
        "        # xcxc I'm assuming that the images live in _out.\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.goal_images = goal_images\n",
        "        self.delta = delta\n",
        "        self.load_as_grayscale = load_as_grayscale\n",
        "        self.df = self._get_dataframe()\n",
        "    \n",
        "    def __len__(self):\n",
        "        num_rows, _ = self.df.shape\n",
        "        return num_rows\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Generate one sample of data.\n",
        "        '''\n",
        "        row = self.df.iloc[idx]\n",
        "        ctr1 = float(row['ctr1'])\n",
        "        ctr2 = float(row['ctr2'])\n",
        "        control_inputs = np.array([ctr1, ctr2])\n",
        "        src_img = self._load_image_and_maybe_apply_transform(row['src'])\n",
        "        tgt_img = self._load_image_and_maybe_apply_transform(row['tgt'])\n",
        "        return (src_img, tgt_img, control_inputs)\n",
        "\n",
        "    def _load_image_and_maybe_apply_transform(self, filename):\n",
        "        '''\n",
        "        Inputs:\n",
        "            image_loc: The location of the image we want to load\n",
        "        Outputs:\n",
        "            Either the grayscale image, a RGB image with the axes flopped, or \n",
        "            the RGB image with some series of transformations applied. \n",
        "            All are converted to numpy arrays before yeeting them out.\n",
        "        '''\n",
        "        # I've been writing too much haskell\n",
        "        image_loc = os.path.join(self.data_dir, '_out', filename)\n",
        "        pil_img = Image.open(image_loc)\n",
        "        if self.load_as_grayscale:\n",
        "            pil_img = pil_img.convert('L')\n",
        "        \n",
        "        if self.transform:\n",
        "            transform_result = self.transform(pil_img)\n",
        "            return np.asarray(transform_result[:3, :, :])\n",
        "        else:\n",
        "            if self.load_as_grayscale:\n",
        "                return np.array(pil_img)\n",
        "            else:\n",
        "                return self._rearrange_axes_image(np.array(pil_img))\n",
        "\n",
        "    def _rearrange_axes_image(self, img):\n",
        "        H,W,_ = img.shape\n",
        "        new_img = np.zeros((3,H,W))\n",
        "        for i in range(3):\n",
        "            new_img[i,:,:] = img[:,:,i]\n",
        "        return new_img\n",
        "\n",
        "    def _get_dataframe(self):\n",
        "        control_input_df = self._get_control_input_df()\n",
        "        control_input_df['input_num'] = control_input_df['input_num'].astype('int') \n",
        "        filename_df = self._get_image_path_df()\n",
        "        pairwise_df = self._get_pairwise_df(filename_df)\n",
        "        pairwise_df['index'] = pairwise_df['index'].astype('int')\n",
        "        df = control_input_df.merge(right=pairwise_df,\n",
        "                                    left_on=['input_num', 'trajectory'],\n",
        "                                    right_on=['index', 'trajectory'])\n",
        "        stationary_mask = (df['src'] == df['tgt'])\n",
        "        ctr1_col = df['ctr1'].copy()\n",
        "        ctr2_col = df['ctr2'].copy()\n",
        "        ctr1_col[stationary_mask] = 0\n",
        "        ctr2_col[stationary_mask] = 0\n",
        "        df['ctr1'] = ctr1_col\n",
        "        df['ctr2'] = ctr2_col\n",
        "        df = df[['trajectory', 'index', 'ctr1', 'ctr2', 'src', 'tgt']]\n",
        "        return df.drop_duplicates()\n",
        "\n",
        "    def _get_control_input_df(self):\n",
        "        # xcxc I'm also assuming that our columns in control_input stay static like so.\n",
        "        control_input_df = pd.read_csv(os.path.join(self.data_dir, 'control_input.txt'),\n",
        "                               names=['trajectory', 'input_num', 'ctr1', 'ctr2'])\n",
        "        control_input_df['input_num'] = control_input_df['input_num'].astype('str')\n",
        "        control_input_df['trajectory'] =control_input_df['trajectory'].astype('str')\n",
        "        return control_input_df\n",
        "    \n",
        "    def _get_image_path_df(self):\n",
        "        '''\n",
        "        Different from the OG CarlaDS.\n",
        "        This returns a dataframe of the \n",
        "        '''\n",
        "        all_files_in_out = self._get_image_files_in_directory()\n",
        "        # We can then make a map with our data...\n",
        "        filename_groupings = {}\n",
        "        for fn in all_files_in_out:\n",
        "            # Apologies for the hardcoding\n",
        "            fn_number = str(int(fn.split('_')[0]))\n",
        "            trajectory_number = str(int(fn.split('_')[2].split('.')[0]))\n",
        "            if (fn_number, trajectory_number) not in filename_groupings:\n",
        "                filename_groupings[(fn_number, trajectory_number)] = []\n",
        "            filename_groupings[(fn_number, trajectory_number)].append(fn)\n",
        "            \n",
        "        # Then make a dataframe from this dictionary\n",
        "        filename_df = self._get_initial_filename_dataframe(filename_groupings)\n",
        "        return filename_df\n",
        "    \n",
        "    def _get_initial_filename_dataframe(self, filename_groupings):\n",
        "        '''\n",
        "        Given the filename groupings from the above, create a dataframe\n",
        "        of the schema [trajectory, index, image1, image2]\n",
        "        '''\n",
        "        filename_df = pd.DataFrame(columns=['trajectory', 'index', 'src'])\n",
        "        for k,v in filename_groupings.items():\n",
        "            (index, traj) = k\n",
        "            img1 = None\n",
        "            if len(v) == 1:\n",
        "                img1 = v[0]\n",
        "            filename_df = filename_df.append({\n",
        "                'trajectory': traj,\n",
        "                'index': index,\n",
        "                'src': img1\n",
        "            }, ignore_index=True)\n",
        "        filename_df['trajectory'] = filename_df['trajectory'].astype('str')\n",
        "        filename_df['index'] = filename_df['index'].astype('int')\n",
        "        filename_df = filename_df.dropna(subset=['src']) # Drop if any of our images is None.\n",
        "        return filename_df\n",
        "    \n",
        "    def _get_pairwise_df(self, filename_df):\n",
        "        pairwise_df = pd.DataFrame(columns=['trajectory', 'index', 'src', 'tgt'])\n",
        "        trajectory_map = self._construct_trajectory_map(filename_df)\n",
        "        for trajectory, goal_fn in trajectory_map.items():\n",
        "            fn_subset_df = filename_df[filename_df['trajectory']==trajectory]\n",
        "            pairwise_df = self._get_pairwise_combinations_for_goal(\n",
        "                goal_fn, fn_subset_df, pairwise_df)\n",
        "        pairwise_df['trajectory'] = pairwise_df['trajectory'].astype('str')\n",
        "        pairwise_df['index'] = pairwise_df['index'].astype('str')\n",
        "        return pairwise_df\n",
        "    \n",
        "    def _construct_trajectory_map(self, filename_df):\n",
        "        '''\n",
        "        Constructs a map such that\n",
        "        {trajectory: goal}\n",
        "        So then it's just a matter of iterating through this map.\n",
        "        '''\n",
        "        if len(self.goal_images) > 0:\n",
        "            return self.goal_images\n",
        "        trajectories = filename_df['trajectory'].unique().tolist()\n",
        "        def helper(traj):\n",
        "            return filename_df[filename_df['trajectory']==traj]['src'].max()\n",
        "            \n",
        "        goal_filenames = map(lambda t: helper(t), trajectories)\n",
        "        goal_filenames = list(goal_filenames)\n",
        "        return {trajectories[i]: goal_filenames[i] for i in range(len(trajectories))}\n",
        "    \n",
        "    def _get_pairwise_combinations_for_goal(self, goal_image, filename_df, pairwise_df):\n",
        "        '''\n",
        "        With filename_df, we construct the ('index', 'src', 'tgt' here), constructed by \n",
        "        '''\n",
        "        num_rows, _ = filename_df.shape\n",
        "        tgt_index = int(goal_image.split('_')[0]) # Get which # image we want to go up to\n",
        "        \n",
        "        for i in range(num_rows):\n",
        "            # Get data from our current row\n",
        "            ith_row = filename_df.iloc[i]\n",
        "            index = int(ith_row['index'])\n",
        "            # Get all the potential target images\n",
        "            src_filename = ith_row['src']\n",
        "            timestep = 1 # images increment by 1\n",
        "            indices = list(np.arange(index, tgt_index, self.delta * timestep)) # Hardcoding in 4 because images increment by 4\n",
        "            if self.delta != 1:\n",
        "                indices.append(index + timestep) # And to get t+1 as well.\n",
        "            tgt_rows = filename_df[filename_df['index'].astype('int').isin(indices)] # Get all the target rows\n",
        "            # Then loop through our filenames and pair them together and append them to our df\n",
        "            for tgt_filename in tgt_rows['src']:\n",
        "                pairwise_df = pairwise_df.append({\n",
        "                    'trajectory': ith_row['trajectory'],\n",
        "                    'index': index,\n",
        "                    'src': src_filename,\n",
        "                    'tgt': tgt_filename\n",
        "                }, ignore_index=True)\n",
        "        return pairwise_df\n",
        "    \n",
        "    def _get_image_files_in_directory(self, end='png'):\n",
        "        '''\n",
        "        Retrieves all the filenames in the data directory with some end extension.\n",
        "        Currently, end is png.\n",
        "        '''\n",
        "        full_data = glob.glob(os.path.join(self.data_dir, '_out', '**.' + end))\n",
        "        abbrev_data = [x.split('/')[-1] for x in full_data]\n",
        "        return abbrev_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciYP-EuwPgDA",
        "colab_type": "text"
      },
      "source": [
        "### Model: `velocityNN.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6mPImzQRFK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class velocityNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64,512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.2))\n",
        "        \n",
        "        self.vel = nn.Linear(512,6)\n",
        "        self.steer = nn.Linear(512,11)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc(x)\n",
        "        vel = self.vel(h)\n",
        "        steer = self.steer(h)\n",
        "        # vel = F.softmax(vel, dim=6)\n",
        "        # vel = F.softmax(steer, dim=11)\n",
        "        return vel.unsqueeze(0) , steer.unsqueeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr_pa3iVlHJy",
        "colab_type": "text"
      },
      "source": [
        "### Model: `siameseCVAE.py` (xcxc To be changed later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMICuhAklF3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class siameseCVAE(nn.Module):\n",
        "\tdef __init__(self,batch=4):\n",
        "\t\tsuper().__init__()\n",
        "\t\td = 0.4\n",
        "\t\tself.z_size = 64\n",
        "\t\tself.small = 256\n",
        "\t\tself.hidden = 1024\n",
        "\t\tch_sz = 1\n",
        "\t\tc1 = 4\n",
        "\t\tc2 = 16\n",
        "\t\tlast_conv = 16\n",
        "\t\tself.tensor = (batch,last_conv,75,100)\n",
        "\t\tflat = np.prod(self.tensor)\n",
        "\t\tflat2 = flat*2\n",
        "\n",
        "\t\t# channel_in, c_out, kernel_size, stride, padding\n",
        "\t\tdef convbn(ci,co,ksz,s=1,pz=0):\t\t#ReLu nonlinearity\n",
        "\t\t\treturn nn.Sequential(\n",
        "\t\t\t\tnn.Conv2d(ci,co,ksz,stride=s,padding=pz),\n",
        "\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\tnn.BatchNorm2d(co))\n",
        "\t\tdef convout(ci,co,ksz,s=1,pz=0):\t#Sigmoid nonlinearity\n",
        "\t\t\treturn nn.Sequential(\n",
        "\t\t\t\tnn.Conv2d(ci,co,ksz,stride=s,padding=pz),\n",
        "\t\t\t\tnn.Sigmoid(),\n",
        "\t\t\t\tnn.BatchNorm2d(co))\n",
        "\t\tdef mlp(in_size,hidden):\n",
        "\t\t\treturn nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tnn.Linear(in_size,hidden),\n",
        "\t\t\t\tnn.ReLU())\n",
        "\n",
        "\t\t#Encoder NN\n",
        "\t\tself.encx = nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tconvbn(ch_sz,c1,3,1,1),\n",
        "\t\t\t\tconvbn(c1,c2,3,1,1),\n",
        "\t\t\t\tconvbn(c2,last_conv,3,1,1))\n",
        "\t\tself.ency = nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tconvbn(ch_sz,c1,3,1,1),\n",
        "\t\t\t\tconvbn(c1,c2,3,1,1),\n",
        "\t\t\t\tconvbn(c2,last_conv,3,1,1))\n",
        "\t\tself.m1 = nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tmlp(flat2,self.hidden),\n",
        "\t\t\t\tmlp(self.hidden, self.small))\n",
        "\t\tself.zmean = nn.Linear(self.small,self.z_size)\n",
        "\t\tself.zlogvar = nn.Linear(self.small,self.z_size)\n",
        "\n",
        "\t\t#Decoder NN\n",
        "\t\tself.expand_z = nn.Linear(self.z_size,self.small)\n",
        "\t\tself.mx = nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tmlp(self.small,self.hidden),\n",
        "\t\t\t\tmlp(self.hidden,flat))\n",
        "\t\tself.my = nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tmlp(self.small,self.hidden),\n",
        "\t\t\t\tmlp(self.hidden,flat))\n",
        "\t\tself.decx = nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tconvbn(last_conv,c2,3,1,1),\n",
        "\t\t\t\tconvbn(c2,c1,3,1,1),\n",
        "\t\t\t\tconvout(c1,ch_sz,3,1,1))\n",
        "\t\tself.decy = nn.Sequential(\n",
        "\t\t\t\tnn.Dropout(d),\n",
        "\t\t\t\tconvbn(last_conv,c2,3,1,1),\n",
        "\t\t\t\tconvbn(c2,c1,3,1,1),\n",
        "\t\t\t\tconvout(c1,ch_sz,3,1,1))\n",
        "\n",
        "\tdef encoder(self, x, y):\n",
        "\t\t# Flatten enc output\n",
        "\t\th_x = self.encx(x).view(-1)\n",
        "\t\th_y = self.ency(y).view(-1)\n",
        "\t\t# Concatenate flat convs\n",
        "\t\th_layer = torch.cat((h_x,h_y))\n",
        "\t\th = self.m1(h_layer)\n",
        "\t\treturn h\n",
        "\n",
        "\tdef bottleneck(self, x):\n",
        "\t\tz_mean = self.zmean(x)\n",
        "\t\tz_logvar = self.zlogvar(x)\n",
        "\t\t#reparam to get z latent sample\n",
        "\t\tstd = torch.exp(0.5*z_logvar)\n",
        "\t\teps = torch.randn_like(std)\n",
        "\t\tz = z_mean + eps*std\n",
        "\t\treturn z, z_mean, z_logvar\n",
        "\n",
        "\tdef decoder(self, z):\n",
        "\t\t#check the nonlinearities of this layer\n",
        "\t\th = self.expand_z(z)\n",
        "\t\t#exand z to each decoder head\n",
        "\t\th_x = self.mx(h)\n",
        "\t\th_y = self.my(h)\n",
        "\t\t#make sure to reshape data correctly and decode\n",
        "\t\tx = self.decx(h_x.view(self.tensor))\n",
        "\t\ty = self.decy(h_x.view(self.tensor))\n",
        "\t\treturn x, y\n",
        "\n",
        "\tdef forward(self, x, y):\n",
        "\t\th = self.encoder(x, y)\n",
        "\t\tz, z_mean, z_logvar = self.bottleneck(h)\n",
        "\t\tx_hat, y_hat = self.decoder(z)\n",
        "\t\treturn x_hat, y_hat, z, z_mean, z_logvar\n",
        "\n",
        "\tdef encode_get_z(self, x, y):\n",
        "\t\th = self.encoder(x, y)\n",
        "\t\tz, z_mean, z_logvar = self.bottleneck(h)\n",
        "\t\treturn z, z_mean, z_logvar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moTnnTc5lkBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _rearrange_channel_last(img, color = False):\n",
        "    _,_,H,W = img.shape\n",
        "    if color == True:\n",
        "        new_img = np.zeros((H,W,3))\n",
        "        for i in range(3):\n",
        "            new_img[:,:,i] = img[0,i,:,:]\n",
        "    else:\n",
        "        new_img = np.zeros((H,W))\n",
        "        new_img[:,:] = img[0,0,:,:]\n",
        "    return new_img\n",
        "\n",
        "def ELBO_loss(xhat, x, yhat, y, mu, logvar):\n",
        "    mseloss = nn.MSELoss(reduction='sum')\n",
        "    MSE_X = mseloss(xhat, x)\n",
        "    MSE_Y = mseloss(yhat, y)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return MSE_X+MSE_Y+KLD, MSE_X, MSE_Y, KLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Uv8T5DlwcP",
        "colab_type": "text"
      },
      "source": [
        "### Training Script: `train_velModel.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCDXdR_CJABE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainVAE(net, optimizer, criterion, epochs, dataloader, exp_name):\n",
        "    model = net.to(device)\n",
        "    total_step = len(dataloader)\n",
        "    overall_step = 0\n",
        "    losses, kl_loss, mseX_loss, mseY_loss = [], [], [], []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total = 0 \n",
        "        running_loss, kl_running, mseX_running, mseY_running = 0.0, 0.0, 0.0, 0.0\n",
        "        for i, X in enumerate(dataloader):\n",
        "            t0 = X[0].float().to(device)\n",
        "            tk = X[1].float().to(device)\n",
        "\n",
        "            xhat, yhat, z, z_mean, z_logvar = model.forward(t0,tk)\n",
        "            loss, MSE_X, MSE_Y, KLD = criterion(xhat,t0, yhat, tk, z_mean, z_logvar)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            kl_running += KLD.item()\n",
        "            mseX_running += MSE_X.item()\n",
        "            mseY_running += MSE_Y.item()\n",
        "            total += X[2].size(0)\n",
        "\n",
        "            if (i+1) % 10 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "            if i == 25:\n",
        "                break\n",
        "        \n",
        "        if (epoch+1) % 10 == 0:\n",
        "            chpt_path = base+'checkpoints/'+exp_name+'.pt'\n",
        "            torch.save(model.state_dict(), chpt_path)\n",
        "\n",
        "        losses.append(running_loss/total)     \n",
        "        kl_loss.append(kl_running/total)\n",
        "        mseX_loss.append(mseX_running/total)\n",
        "        mseY_loss.append(mseY_running/total)  \n",
        "    \n",
        "    ells = {'elbo':losses,\n",
        "            'kl':kl_loss,\n",
        "            'mseX':mseX_loss,\n",
        "            'mseY':mseY_loss}\n",
        "\n",
        "    with open(base+'logs/'+exp_name+'_losses.pickle', 'wb') as f:\n",
        "        pickle.dump(ells, f)\n",
        "\n",
        "    return ells       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uphtXd6dU9t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testVAE(net, criterion, dataloader):\n",
        "    t0_list, tk_list = [], []\n",
        "    recon_t0_list, recon_tk_list = [], []\n",
        "    for i, X in enumerate(dataloader):\n",
        "        model.eval()\n",
        "        t0 = X[0].float().to(device)\n",
        "        tk = X[1].float().to(device)\n",
        "        u = X[2].float().to(device)\n",
        "\n",
        "        #Forward Pass\n",
        "        xhat, yhat, z, z_mean, z_stdev = model.forward(t0,tk)\n",
        "\n",
        "        t0_ = t0.cpu().squeeze().numpy()\n",
        "        tk_ = tk.cpu().squeeze().numpy()\n",
        "        xhat_ = xhat.cpu().detach().squeeze().numpy()\n",
        "        yhat_ = yhat.cpu().detach().squeeze().numpy()\n",
        "\n",
        "        t0_list.append(t0_)\n",
        "        tk_list.append(tk_)\n",
        "        recon_t0_list.append(xhat_)\n",
        "        recon_tk_list.append(yhat_)\n",
        "        if i == 40:\n",
        "            break\n",
        "\n",
        "    t0_list = np.asarray(t0_list)\n",
        "    tk_list = np.asarray(tk_list)\n",
        "    recon_t0_list = np.asarray(recon_t0_list)\n",
        "    recon_tk_list = np.asarray(recon_tk_list)\n",
        "\n",
        "    result = {'t0':t0_list, 'tk':tk_list, 'recon_t0':recon_t0_list, 'recon_tk':recon_tk_list}\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX0w5UbalWu1",
        "colab_type": "code",
        "outputId": "351e3ac6-095f-4eb1-878c-087ea4b212bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.Resize((75,100)),\n",
        "        transforms.ToTensor()])\n",
        "\n",
        "batch = 1\n",
        "path = base + \"project_data/synced_single_camera/\"\n",
        "\n",
        "print(os.listdir(base))\n",
        "\n",
        "dl = DataLoader(VelocityPredictionCarlaDataSet(path, load_as_grayscale=True, transform=transform), batch_size=batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['project_data', 'ESE546 Roadmap.gdoc', 'siameseCVAE.ipynb', 'checkpoints', 'logs', 'velocityModel.ipynb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu7RKuq1MZCe",
        "colab_type": "code",
        "outputId": "8b60afe3-7d39-47e9-96eb-1116fb8443eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Run from here\n",
        "exp_name = 'siamese_single_test'\n",
        "model = siameseCVAE(batch=batch)\n",
        "\n",
        "epochs = 1\n",
        "criterion = ELBO_loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-3)\n",
        "\n",
        "ells = trainVAE(model, optimizer, criterion, epochs, dl, exp_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/1], Step [10/2398], Loss: 51365.7305\n",
            "Epoch [1/1], Step [20/2398], Loss: 49199.1133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuqriY8OLSgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(ells['elbo'], label='ELBO')\n",
        "plt.plot(ells['kl'], label='KL')\n",
        "plt.plot(ells['mseX'], label='MSE')\n",
        "plt.plot(ells['mseY'], label='MSE')\n",
        "plt.title('Train loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaFak1rLWHg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = testVAE(model, criterion, dl):\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(221)\n",
        "plt.imshow(result['t0'][0], cmap = 'gray')\n",
        "plt.subplot(222)\n",
        "plt.imshow(result['tk'][0], cmap = 'gray')\n",
        "plt.subplot(223)\n",
        "plt.imshow(result['recon_t0'][0], cmap = 'gray')\n",
        "plt.subplot(224)\n",
        "plt.imshow(result['recon_tk'][0], cmap = 'gray')\n",
        "plt.figure()\n",
        "plt.subplot(221)\n",
        "plt.imshow(result['t0'][30], cmap = 'gray')\n",
        "plt.subplot(222)\n",
        "plt.imshow(result['tk'][30], cmap = 'gray')\n",
        "plt.subplot(223)\n",
        "plt.imshow(result['recon_t0'][30], cmap = 'gray')\n",
        "plt.subplot(224)\n",
        "plt.imshow(result['recon_tk'][30], cmap = 'gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V71As9S3hUaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input2classification(control, lb, ub, num_class):\n",
        "    interval_list = torch.linspace(lb,ub,num_class).to(device)\n",
        "    val = abs(interval_list-control.unsqueeze(1))\n",
        "    idx = torch.argmin(val, axis=1)\n",
        "    label = torch.zeros((control.shape[0],interval_list.shape[0])).to(device)\n",
        "    \n",
        "    i = np.arange(0, control.shape[0])\n",
        "    i = torch.from_numpy(i).to(device)\n",
        "\n",
        "    label[i,idx] = 1\n",
        "    return idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwnPAnLQl8yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainVel(netVAE, netVel, optimizer, criterion, epochs, dataloader, exp_name):\n",
        "    modelVAE = netVAE.to(device)\n",
        "    modelVel = netVel.to(device)\n",
        "    total_step = len(dataloader)\n",
        "    overall_step = 0\n",
        "    accuracy, loss_list = [], []\n",
        "    for epoch in range(epochs):\n",
        "        modelVel.train()\n",
        "        modelVAE.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        total_loss = 0\n",
        "        for i, X in enumerate(dataloader):\n",
        "            t0 = X[0].float().to(device)\n",
        "            tk = X[1].float().to(device)\n",
        "            u = X[2].float().to(device)\n",
        "\n",
        "            label_vel = input2classification(u[:,1],0,50,6)\n",
        "            label_steer = input2classification(u[:,0],-1,1,11)\n",
        "\n",
        "            #Forward Pass\n",
        "            xhat, yhat, z, z_mean, z_stdev = modelVAE.forward(t0,tk)\n",
        "            vel, steer = modelVel.forward(z)\n",
        "\n",
        "            loss = criterion(vel, label_vel) + criterion(steer, label_steer) \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(vel.data, 1)\n",
        "            total += label_vel.size(0)\n",
        "            correct += (predicted == label_vel).sum().item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            acc = 100*(predicted == label_vel).sum().item()/label_vel.size(0)\n",
        "            overall_step += 1\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {}'.format(epoch+1, epochs, overall_step, total_step*epochs, loss.item(), acc))\n",
        "            \n",
        "        if (epoch+1) % 5 == 0:\n",
        "            chpt_path = base+'checkpoints/'+exp_name+'.pt'\n",
        "            torch.save(modelVel.state_dict(), chpt_path)\n",
        "\n",
        "        accuracy.append(correct/total)\n",
        "        loss_list.append(total_loss/total)\n",
        "\n",
        "    ells = {'accuracy':accuracy,'loss':loss_list}\n",
        "\n",
        "    return ells"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw5poYhyH0gx",
        "colab_type": "code",
        "outputId": "bd114872-b0a4-4c33-ff1a-f3c5f8988ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Run from here\n",
        "modelVAE = siameseCVAE(batch=batch)\n",
        "checkpoint = torch.load(base+'checkpoints/siamese_predict41616_75100.pt')\n",
        "modelVAE.load_state_dict(checkpoint) \n",
        "\n",
        "\n",
        "exp_name = 'velocity_single_test'\n",
        "modelVel = velocityNN()\n",
        "\n",
        "epochs = 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(modelVel.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-3)\n",
        "\n",
        "ells = trainVel(modelVAE, modelVel, optimizer, criterion, epochs, dl, exp_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [100/47960], Loss: 0.0227, Accuracy: 100.0\n",
            "Epoch [1/20], Step [200/47960], Loss: 4.9142, Accuracy: 0.0\n",
            "Epoch [1/20], Step [300/47960], Loss: 0.0261, Accuracy: 100.0\n",
            "Epoch [1/20], Step [400/47960], Loss: 0.7384, Accuracy: 100.0\n",
            "Epoch [1/20], Step [500/47960], Loss: 1.3703, Accuracy: 0.0\n",
            "Epoch [1/20], Step [600/47960], Loss: 21.1606, Accuracy: 0.0\n",
            "Epoch [1/20], Step [700/47960], Loss: 0.7457, Accuracy: 100.0\n",
            "Epoch [1/20], Step [800/47960], Loss: 0.3427, Accuracy: 100.0\n",
            "Epoch [1/20], Step [900/47960], Loss: 3.5047, Accuracy: 0.0\n",
            "Epoch [1/20], Step [1000/47960], Loss: 2.5650, Accuracy: 0.0\n",
            "Epoch [1/20], Step [1100/47960], Loss: 12.1217, Accuracy: 100.0\n",
            "Epoch [1/20], Step [1200/47960], Loss: 4.5701, Accuracy: 100.0\n",
            "Epoch [1/20], Step [1300/47960], Loss: 4.2491, Accuracy: 0.0\n",
            "Epoch [1/20], Step [1400/47960], Loss: 5.7202, Accuracy: 0.0\n",
            "Epoch [1/20], Step [1500/47960], Loss: 1.2905, Accuracy: 100.0\n",
            "Epoch [1/20], Step [1600/47960], Loss: 0.4743, Accuracy: 100.0\n",
            "Epoch [1/20], Step [1700/47960], Loss: 4.0921, Accuracy: 0.0\n",
            "Epoch [1/20], Step [1800/47960], Loss: 1.9565, Accuracy: 100.0\n",
            "Epoch [1/20], Step [1900/47960], Loss: 1.3048, Accuracy: 0.0\n",
            "Epoch [1/20], Step [2000/47960], Loss: 3.9646, Accuracy: 100.0\n",
            "Epoch [1/20], Step [2100/47960], Loss: 4.5746, Accuracy: 0.0\n",
            "Epoch [1/20], Step [2200/47960], Loss: 3.6100, Accuracy: 100.0\n",
            "Epoch [1/20], Step [2300/47960], Loss: 4.4865, Accuracy: 0.0\n",
            "Epoch [2/20], Step [2498/47960], Loss: 1.2121, Accuracy: 0.0\n",
            "Epoch [2/20], Step [2598/47960], Loss: 1.1318, Accuracy: 0.0\n",
            "Epoch [2/20], Step [2698/47960], Loss: 0.8262, Accuracy: 0.0\n",
            "Epoch [2/20], Step [2798/47960], Loss: 0.3021, Accuracy: 100.0\n",
            "Epoch [2/20], Step [2898/47960], Loss: 1.5207, Accuracy: 0.0\n",
            "Epoch [2/20], Step [2998/47960], Loss: 19.0674, Accuracy: 0.0\n",
            "Epoch [2/20], Step [3098/47960], Loss: 1.4404, Accuracy: 100.0\n",
            "Epoch [2/20], Step [3198/47960], Loss: 0.8742, Accuracy: 100.0\n",
            "Epoch [2/20], Step [3298/47960], Loss: 3.3684, Accuracy: 0.0\n",
            "Epoch [2/20], Step [3398/47960], Loss: 1.8415, Accuracy: 100.0\n",
            "Epoch [2/20], Step [3498/47960], Loss: 9.3115, Accuracy: 100.0\n",
            "Epoch [2/20], Step [3598/47960], Loss: 4.5321, Accuracy: 0.0\n",
            "Epoch [2/20], Step [3698/47960], Loss: 3.4235, Accuracy: 100.0\n",
            "Epoch [2/20], Step [3798/47960], Loss: 5.3017, Accuracy: 0.0\n",
            "Epoch [2/20], Step [3898/47960], Loss: 2.8590, Accuracy: 100.0\n",
            "Epoch [2/20], Step [3998/47960], Loss: 0.4007, Accuracy: 100.0\n",
            "Epoch [2/20], Step [4098/47960], Loss: 4.2255, Accuracy: 0.0\n",
            "Epoch [2/20], Step [4198/47960], Loss: 2.8668, Accuracy: 100.0\n",
            "Epoch [2/20], Step [4298/47960], Loss: 2.7576, Accuracy: 100.0\n",
            "Epoch [2/20], Step [4398/47960], Loss: 4.1844, Accuracy: 100.0\n",
            "Epoch [2/20], Step [4498/47960], Loss: 4.4947, Accuracy: 0.0\n",
            "Epoch [2/20], Step [4598/47960], Loss: 4.0601, Accuracy: 0.0\n",
            "Epoch [2/20], Step [4698/47960], Loss: 4.4916, Accuracy: 0.0\n",
            "Epoch [3/20], Step [4896/47960], Loss: 2.4775, Accuracy: 100.0\n",
            "Epoch [3/20], Step [4996/47960], Loss: 1.4782, Accuracy: 0.0\n",
            "Epoch [3/20], Step [5096/47960], Loss: 0.7556, Accuracy: 0.0\n",
            "Epoch [3/20], Step [5196/47960], Loss: 0.7717, Accuracy: 100.0\n",
            "Epoch [3/20], Step [5296/47960], Loss: 3.4256, Accuracy: 0.0\n",
            "Epoch [3/20], Step [5396/47960], Loss: 15.0070, Accuracy: 0.0\n",
            "Epoch [3/20], Step [5496/47960], Loss: 0.7830, Accuracy: 100.0\n",
            "Epoch [3/20], Step [5596/47960], Loss: 0.0011, Accuracy: 100.0\n",
            "Epoch [3/20], Step [5696/47960], Loss: 1.8496, Accuracy: 0.0\n",
            "Epoch [3/20], Step [5796/47960], Loss: 1.6556, Accuracy: 100.0\n",
            "Epoch [3/20], Step [5896/47960], Loss: 6.2700, Accuracy: 100.0\n",
            "Epoch [3/20], Step [5996/47960], Loss: 4.8835, Accuracy: 0.0\n",
            "Epoch [3/20], Step [6096/47960], Loss: 2.8744, Accuracy: 100.0\n",
            "Epoch [3/20], Step [6196/47960], Loss: 4.8929, Accuracy: 0.0\n",
            "Epoch [3/20], Step [6296/47960], Loss: 2.2275, Accuracy: 100.0\n",
            "Epoch [3/20], Step [6396/47960], Loss: 0.5574, Accuracy: 100.0\n",
            "Epoch [3/20], Step [6496/47960], Loss: 4.4615, Accuracy: 0.0\n",
            "Epoch [3/20], Step [6596/47960], Loss: 4.3896, Accuracy: 0.0\n",
            "Epoch [3/20], Step [6696/47960], Loss: 1.4179, Accuracy: 0.0\n",
            "Epoch [3/20], Step [6796/47960], Loss: 3.7056, Accuracy: 100.0\n",
            "Epoch [3/20], Step [6896/47960], Loss: 4.7850, Accuracy: 0.0\n",
            "Epoch [3/20], Step [6996/47960], Loss: 4.2105, Accuracy: 0.0\n",
            "Epoch [3/20], Step [7096/47960], Loss: 4.7364, Accuracy: 0.0\n",
            "Epoch [4/20], Step [7294/47960], Loss: 0.6522, Accuracy: 100.0\n",
            "Epoch [4/20], Step [7394/47960], Loss: 1.0389, Accuracy: 0.0\n",
            "Epoch [4/20], Step [7494/47960], Loss: 0.6484, Accuracy: 100.0\n",
            "Epoch [4/20], Step [7594/47960], Loss: 2.0774, Accuracy: 0.0\n",
            "Epoch [4/20], Step [7694/47960], Loss: 3.0458, Accuracy: 0.0\n",
            "Epoch [4/20], Step [7794/47960], Loss: 13.3841, Accuracy: 0.0\n",
            "Epoch [4/20], Step [7894/47960], Loss: 1.0317, Accuracy: 100.0\n",
            "Epoch [4/20], Step [7994/47960], Loss: 1.7331, Accuracy: 100.0\n",
            "Epoch [4/20], Step [8094/47960], Loss: 8.4342, Accuracy: 0.0\n",
            "Epoch [4/20], Step [8194/47960], Loss: 0.9588, Accuracy: 100.0\n",
            "Epoch [4/20], Step [8294/47960], Loss: 5.4432, Accuracy: 100.0\n",
            "Epoch [4/20], Step [8394/47960], Loss: 5.4865, Accuracy: 0.0\n",
            "Epoch [4/20], Step [8494/47960], Loss: 4.4085, Accuracy: 0.0\n",
            "Epoch [4/20], Step [8594/47960], Loss: 5.1346, Accuracy: 0.0\n",
            "Epoch [4/20], Step [8694/47960], Loss: 0.9930, Accuracy: 100.0\n",
            "Epoch [4/20], Step [8794/47960], Loss: 1.6583, Accuracy: 100.0\n",
            "Epoch [4/20], Step [8894/47960], Loss: 4.7164, Accuracy: 0.0\n",
            "Epoch [4/20], Step [8994/47960], Loss: 4.4349, Accuracy: 0.0\n",
            "Epoch [4/20], Step [9094/47960], Loss: 1.6886, Accuracy: 100.0\n",
            "Epoch [4/20], Step [9194/47960], Loss: 3.6925, Accuracy: 100.0\n",
            "Epoch [4/20], Step [9294/47960], Loss: 5.1801, Accuracy: 0.0\n",
            "Epoch [4/20], Step [9394/47960], Loss: 4.3003, Accuracy: 0.0\n",
            "Epoch [4/20], Step [9494/47960], Loss: 4.9384, Accuracy: 0.0\n",
            "Epoch [5/20], Step [9692/47960], Loss: 0.6303, Accuracy: 100.0\n",
            "Epoch [5/20], Step [9792/47960], Loss: 0.8980, Accuracy: 0.0\n",
            "Epoch [5/20], Step [9892/47960], Loss: 1.1675, Accuracy: 0.0\n",
            "Epoch [5/20], Step [9992/47960], Loss: 1.3480, Accuracy: 0.0\n",
            "Epoch [5/20], Step [10092/47960], Loss: 1.3860, Accuracy: 0.0\n",
            "Epoch [5/20], Step [10192/47960], Loss: 15.1386, Accuracy: 0.0\n",
            "Epoch [5/20], Step [10292/47960], Loss: 0.6498, Accuracy: 100.0\n",
            "Epoch [5/20], Step [10392/47960], Loss: 1.5129, Accuracy: 100.0\n",
            "Epoch [5/20], Step [10492/47960], Loss: 4.4390, Accuracy: 0.0\n",
            "Epoch [5/20], Step [10592/47960], Loss: 1.4682, Accuracy: 100.0\n",
            "Epoch [5/20], Step [10692/47960], Loss: 4.8637, Accuracy: 100.0\n",
            "Epoch [5/20], Step [10792/47960], Loss: 5.7504, Accuracy: 0.0\n",
            "Epoch [5/20], Step [10892/47960], Loss: 4.5014, Accuracy: 0.0\n",
            "Epoch [5/20], Step [10992/47960], Loss: 5.2980, Accuracy: 0.0\n",
            "Epoch [5/20], Step [11092/47960], Loss: 1.5126, Accuracy: 100.0\n",
            "Epoch [5/20], Step [11192/47960], Loss: 0.8437, Accuracy: 100.0\n",
            "Epoch [5/20], Step [11292/47960], Loss: 4.7458, Accuracy: 0.0\n",
            "Epoch [5/20], Step [11392/47960], Loss: 4.4995, Accuracy: 0.0\n",
            "Epoch [5/20], Step [11492/47960], Loss: 1.1816, Accuracy: 100.0\n",
            "Epoch [5/20], Step [11592/47960], Loss: 3.7420, Accuracy: 100.0\n",
            "Epoch [5/20], Step [11692/47960], Loss: 5.2716, Accuracy: 0.0\n",
            "Epoch [5/20], Step [11792/47960], Loss: 4.3432, Accuracy: 0.0\n",
            "Epoch [5/20], Step [11892/47960], Loss: 5.1739, Accuracy: 0.0\n",
            "Epoch [6/20], Step [12090/47960], Loss: 1.5028, Accuracy: 100.0\n",
            "Epoch [6/20], Step [12190/47960], Loss: 0.8629, Accuracy: 100.0\n",
            "Epoch [6/20], Step [12290/47960], Loss: 1.8484, Accuracy: 0.0\n",
            "Epoch [6/20], Step [12390/47960], Loss: 1.8371, Accuracy: 0.0\n",
            "Epoch [6/20], Step [12490/47960], Loss: 2.4205, Accuracy: 0.0\n",
            "Epoch [6/20], Step [12590/47960], Loss: 12.3080, Accuracy: 0.0\n",
            "Epoch [6/20], Step [12690/47960], Loss: 0.7009, Accuracy: 100.0\n",
            "Epoch [6/20], Step [12790/47960], Loss: 0.0033, Accuracy: 100.0\n",
            "Epoch [6/20], Step [12890/47960], Loss: 7.9454, Accuracy: 0.0\n",
            "Epoch [6/20], Step [12990/47960], Loss: 1.2912, Accuracy: 100.0\n",
            "Epoch [6/20], Step [13090/47960], Loss: 6.4109, Accuracy: 100.0\n",
            "Epoch [6/20], Step [13190/47960], Loss: 5.9111, Accuracy: 0.0\n",
            "Epoch [6/20], Step [13290/47960], Loss: 4.4772, Accuracy: 0.0\n",
            "Epoch [6/20], Step [13390/47960], Loss: 6.5023, Accuracy: 0.0\n",
            "Epoch [6/20], Step [13490/47960], Loss: 1.5310, Accuracy: 100.0\n",
            "Epoch [6/20], Step [13590/47960], Loss: 0.5184, Accuracy: 100.0\n",
            "Epoch [6/20], Step [13690/47960], Loss: 4.7939, Accuracy: 0.0\n",
            "Epoch [6/20], Step [13790/47960], Loss: 4.4421, Accuracy: 0.0\n",
            "Epoch [6/20], Step [13890/47960], Loss: 1.3307, Accuracy: 0.0\n",
            "Epoch [6/20], Step [13990/47960], Loss: 3.7850, Accuracy: 100.0\n",
            "Epoch [6/20], Step [14090/47960], Loss: 5.4269, Accuracy: 0.0\n",
            "Epoch [6/20], Step [14190/47960], Loss: 4.3100, Accuracy: 0.0\n",
            "Epoch [6/20], Step [14290/47960], Loss: 4.9250, Accuracy: 0.0\n",
            "Epoch [7/20], Step [14488/47960], Loss: 1.5138, Accuracy: 100.0\n",
            "Epoch [7/20], Step [14588/47960], Loss: 1.4898, Accuracy: 100.0\n",
            "Epoch [7/20], Step [14688/47960], Loss: 0.6807, Accuracy: 100.0\n",
            "Epoch [7/20], Step [14788/47960], Loss: 1.0453, Accuracy: 0.0\n",
            "Epoch [7/20], Step [14888/47960], Loss: 3.3202, Accuracy: 0.0\n",
            "Epoch [7/20], Step [14988/47960], Loss: 13.5754, Accuracy: 0.0\n",
            "Epoch [7/20], Step [15088/47960], Loss: 1.3837, Accuracy: 100.0\n",
            "Epoch [7/20], Step [15188/47960], Loss: 0.2513, Accuracy: 100.0\n",
            "Epoch [7/20], Step [15288/47960], Loss: 2.7012, Accuracy: 0.0\n",
            "Epoch [7/20], Step [15388/47960], Loss: 1.2155, Accuracy: 100.0\n",
            "Epoch [7/20], Step [15488/47960], Loss: 6.7251, Accuracy: 100.0\n",
            "Epoch [7/20], Step [15588/47960], Loss: 6.4157, Accuracy: 0.0\n",
            "Epoch [7/20], Step [15688/47960], Loss: 3.9897, Accuracy: 0.0\n",
            "Epoch [7/20], Step [15788/47960], Loss: 5.3573, Accuracy: 0.0\n",
            "Epoch [7/20], Step [15888/47960], Loss: 1.0407, Accuracy: 100.0\n",
            "Epoch [7/20], Step [15988/47960], Loss: 1.1144, Accuracy: 100.0\n",
            "Epoch [7/20], Step [16088/47960], Loss: 1.9537, Accuracy: 100.0\n",
            "Epoch [7/20], Step [16188/47960], Loss: 4.5561, Accuracy: 0.0\n",
            "Epoch [7/20], Step [16288/47960], Loss: 1.0629, Accuracy: 100.0\n",
            "Epoch [7/20], Step [16388/47960], Loss: 3.7742, Accuracy: 100.0\n",
            "Epoch [7/20], Step [16488/47960], Loss: 6.3975, Accuracy: 100.0\n",
            "Epoch [7/20], Step [16588/47960], Loss: 4.3060, Accuracy: 0.0\n",
            "Epoch [7/20], Step [16688/47960], Loss: 5.0818, Accuracy: 0.0\n",
            "Epoch [8/20], Step [16886/47960], Loss: 0.2543, Accuracy: 100.0\n",
            "Epoch [8/20], Step [16986/47960], Loss: 1.5204, Accuracy: 100.0\n",
            "Epoch [8/20], Step [17086/47960], Loss: 1.1220, Accuracy: 0.0\n",
            "Epoch [8/20], Step [17186/47960], Loss: 1.3684, Accuracy: 0.0\n",
            "Epoch [8/20], Step [17286/47960], Loss: 1.6795, Accuracy: 0.0\n",
            "Epoch [8/20], Step [17386/47960], Loss: 8.8979, Accuracy: 0.0\n",
            "Epoch [8/20], Step [17486/47960], Loss: 1.4400, Accuracy: 100.0\n",
            "Epoch [8/20], Step [17586/47960], Loss: 0.5638, Accuracy: 100.0\n",
            "Epoch [8/20], Step [17686/47960], Loss: 7.4311, Accuracy: 0.0\n",
            "Epoch [8/20], Step [17786/47960], Loss: 1.4071, Accuracy: 100.0\n",
            "Epoch [8/20], Step [17886/47960], Loss: 4.0472, Accuracy: 100.0\n",
            "Epoch [8/20], Step [17986/47960], Loss: 5.7139, Accuracy: 0.0\n",
            "Epoch [8/20], Step [18086/47960], Loss: 4.5174, Accuracy: 0.0\n",
            "Epoch [8/20], Step [18186/47960], Loss: 8.1254, Accuracy: 0.0\n",
            "Epoch [8/20], Step [18286/47960], Loss: 1.5221, Accuracy: 100.0\n",
            "Epoch [8/20], Step [18386/47960], Loss: 1.5007, Accuracy: 100.0\n",
            "Epoch [8/20], Step [18486/47960], Loss: 4.8472, Accuracy: 0.0\n",
            "Epoch [8/20], Step [18586/47960], Loss: 3.4797, Accuracy: 0.0\n",
            "Epoch [8/20], Step [18686/47960], Loss: 1.2100, Accuracy: 100.0\n",
            "Epoch [8/20], Step [18786/47960], Loss: 3.7237, Accuracy: 100.0\n",
            "Epoch [8/20], Step [18886/47960], Loss: 5.5518, Accuracy: 0.0\n",
            "Epoch [8/20], Step [18986/47960], Loss: 4.3831, Accuracy: 0.0\n",
            "Epoch [8/20], Step [19086/47960], Loss: 5.0302, Accuracy: 0.0\n",
            "Epoch [9/20], Step [19284/47960], Loss: 1.5049, Accuracy: 100.0\n",
            "Epoch [9/20], Step [19384/47960], Loss: 1.3821, Accuracy: 0.0\n",
            "Epoch [9/20], Step [19484/47960], Loss: 0.8289, Accuracy: 0.0\n",
            "Epoch [9/20], Step [19584/47960], Loss: 2.1656, Accuracy: 0.0\n",
            "Epoch [9/20], Step [19684/47960], Loss: 3.3472, Accuracy: 0.0\n",
            "Epoch [9/20], Step [19784/47960], Loss: 14.6345, Accuracy: 0.0\n",
            "Epoch [9/20], Step [19884/47960], Loss: 1.4216, Accuracy: 100.0\n",
            "Epoch [9/20], Step [19984/47960], Loss: 0.5821, Accuracy: 100.0\n",
            "Epoch [9/20], Step [20084/47960], Loss: 6.0952, Accuracy: 0.0\n",
            "Epoch [9/20], Step [20184/47960], Loss: 1.2360, Accuracy: 100.0\n",
            "Epoch [9/20], Step [20284/47960], Loss: 4.0959, Accuracy: 100.0\n",
            "Epoch [9/20], Step [20384/47960], Loss: 6.0522, Accuracy: 0.0\n",
            "Epoch [9/20], Step [20484/47960], Loss: 4.5643, Accuracy: 0.0\n",
            "Epoch [9/20], Step [20584/47960], Loss: 7.5786, Accuracy: 0.0\n",
            "Epoch [9/20], Step [20684/47960], Loss: 1.1455, Accuracy: 100.0\n",
            "Epoch [9/20], Step [20784/47960], Loss: 1.4382, Accuracy: 100.0\n",
            "Epoch [9/20], Step [20884/47960], Loss: 4.8456, Accuracy: 0.0\n",
            "Epoch [9/20], Step [20984/47960], Loss: 1.8884, Accuracy: 100.0\n",
            "Epoch [9/20], Step [21084/47960], Loss: 1.0851, Accuracy: 100.0\n",
            "Epoch [9/20], Step [21184/47960], Loss: 3.5930, Accuracy: 100.0\n",
            "Epoch [9/20], Step [21284/47960], Loss: 5.6703, Accuracy: 100.0\n",
            "Epoch [9/20], Step [21384/47960], Loss: 4.4270, Accuracy: 0.0\n",
            "Epoch [9/20], Step [21484/47960], Loss: 5.1426, Accuracy: 0.0\n",
            "Epoch [10/20], Step [21682/47960], Loss: 1.3822, Accuracy: 100.0\n",
            "Epoch [10/20], Step [21782/47960], Loss: 1.2839, Accuracy: 0.0\n",
            "Epoch [10/20], Step [21882/47960], Loss: 0.6248, Accuracy: 100.0\n",
            "Epoch [10/20], Step [21982/47960], Loss: 0.1286, Accuracy: 100.0\n",
            "Epoch [10/20], Step [22082/47960], Loss: 3.2793, Accuracy: 0.0\n",
            "Epoch [10/20], Step [22182/47960], Loss: 10.3147, Accuracy: 0.0\n",
            "Epoch [10/20], Step [22282/47960], Loss: 0.7090, Accuracy: 100.0\n",
            "Epoch [10/20], Step [22382/47960], Loss: 0.0099, Accuracy: 100.0\n",
            "Epoch [10/20], Step [22482/47960], Loss: 2.3248, Accuracy: 0.0\n",
            "Epoch [10/20], Step [22582/47960], Loss: 1.3412, Accuracy: 100.0\n",
            "Epoch [10/20], Step [22682/47960], Loss: 9.8756, Accuracy: 100.0\n",
            "Epoch [10/20], Step [22782/47960], Loss: 6.0849, Accuracy: 0.0\n",
            "Epoch [10/20], Step [22882/47960], Loss: 4.6155, Accuracy: 0.0\n",
            "Epoch [10/20], Step [22982/47960], Loss: 5.3230, Accuracy: 0.0\n",
            "Epoch [10/20], Step [23082/47960], Loss: 1.3491, Accuracy: 100.0\n",
            "Epoch [10/20], Step [23182/47960], Loss: 1.4155, Accuracy: 100.0\n",
            "Epoch [10/20], Step [23282/47960], Loss: 3.0997, Accuracy: 100.0\n",
            "Epoch [10/20], Step [23382/47960], Loss: 4.6435, Accuracy: 0.0\n",
            "Epoch [10/20], Step [23482/47960], Loss: 1.1408, Accuracy: 100.0\n",
            "Epoch [10/20], Step [23582/47960], Loss: 3.9180, Accuracy: 100.0\n",
            "Epoch [10/20], Step [23682/47960], Loss: 4.8701, Accuracy: 100.0\n",
            "Epoch [10/20], Step [23782/47960], Loss: 4.4338, Accuracy: 0.0\n",
            "Epoch [10/20], Step [23882/47960], Loss: 5.1171, Accuracy: 0.0\n",
            "Epoch [11/20], Step [24080/47960], Loss: 1.3534, Accuracy: 100.0\n",
            "Epoch [11/20], Step [24180/47960], Loss: 1.3282, Accuracy: 100.0\n",
            "Epoch [11/20], Step [24280/47960], Loss: 1.0329, Accuracy: 0.0\n",
            "Epoch [11/20], Step [24380/47960], Loss: 2.1649, Accuracy: 0.0\n",
            "Epoch [11/20], Step [24480/47960], Loss: 0.3473, Accuracy: 100.0\n",
            "Epoch [11/20], Step [24580/47960], Loss: 20.3016, Accuracy: 0.0\n",
            "Epoch [11/20], Step [24680/47960], Loss: 0.5838, Accuracy: 100.0\n",
            "Epoch [11/20], Step [24780/47960], Loss: 0.0570, Accuracy: 100.0\n",
            "Epoch [11/20], Step [24880/47960], Loss: 1.8398, Accuracy: 0.0\n",
            "Epoch [11/20], Step [24980/47960], Loss: 1.0824, Accuracy: 100.0\n",
            "Epoch [11/20], Step [25080/47960], Loss: 5.3782, Accuracy: 100.0\n",
            "Epoch [11/20], Step [25180/47960], Loss: 6.1410, Accuracy: 0.0\n",
            "Epoch [11/20], Step [25280/47960], Loss: 4.6941, Accuracy: 0.0\n",
            "Epoch [11/20], Step [25380/47960], Loss: 8.2182, Accuracy: 0.0\n",
            "Epoch [11/20], Step [25480/47960], Loss: 1.3731, Accuracy: 100.0\n",
            "Epoch [11/20], Step [25580/47960], Loss: 0.9747, Accuracy: 100.0\n",
            "Epoch [11/20], Step [25680/47960], Loss: 4.9420, Accuracy: 0.0\n",
            "Epoch [11/20], Step [25780/47960], Loss: 4.1593, Accuracy: 0.0\n",
            "Epoch [11/20], Step [25880/47960], Loss: 1.4206, Accuracy: 100.0\n",
            "Epoch [11/20], Step [25980/47960], Loss: 3.8330, Accuracy: 100.0\n",
            "Epoch [11/20], Step [26080/47960], Loss: 6.1929, Accuracy: 100.0\n",
            "Epoch [11/20], Step [26180/47960], Loss: 4.4312, Accuracy: 0.0\n",
            "Epoch [11/20], Step [26280/47960], Loss: 5.1484, Accuracy: 0.0\n",
            "Epoch [12/20], Step [26478/47960], Loss: 1.3781, Accuracy: 100.0\n",
            "Epoch [12/20], Step [26578/47960], Loss: 0.9319, Accuracy: 0.0\n",
            "Epoch [12/20], Step [26678/47960], Loss: 0.6650, Accuracy: 100.0\n",
            "Epoch [12/20], Step [26778/47960], Loss: 0.5492, Accuracy: 100.0\n",
            "Epoch [12/20], Step [26878/47960], Loss: 2.2503, Accuracy: 0.0\n",
            "Epoch [12/20], Step [26978/47960], Loss: 5.9401, Accuracy: 0.0\n",
            "Epoch [12/20], Step [27078/47960], Loss: 0.8901, Accuracy: 100.0\n",
            "Epoch [12/20], Step [27178/47960], Loss: 0.1372, Accuracy: 100.0\n",
            "Epoch [12/20], Step [27278/47960], Loss: 2.3501, Accuracy: 0.0\n",
            "Epoch [12/20], Step [27378/47960], Loss: 0.9717, Accuracy: 100.0\n",
            "Epoch [12/20], Step [27478/47960], Loss: 4.0951, Accuracy: 100.0\n",
            "Epoch [12/20], Step [27578/47960], Loss: 6.1762, Accuracy: 0.0\n",
            "Epoch [12/20], Step [27678/47960], Loss: 4.6666, Accuracy: 0.0\n",
            "Epoch [12/20], Step [27778/47960], Loss: 8.1703, Accuracy: 0.0\n",
            "Epoch [12/20], Step [27878/47960], Loss: 1.0241, Accuracy: 100.0\n",
            "Epoch [12/20], Step [27978/47960], Loss: 1.3690, Accuracy: 100.0\n",
            "Epoch [12/20], Step [28078/47960], Loss: 4.9429, Accuracy: 0.0\n",
            "Epoch [12/20], Step [28178/47960], Loss: 2.5545, Accuracy: 100.0\n",
            "Epoch [12/20], Step [28278/47960], Loss: 1.4238, Accuracy: 100.0\n",
            "Epoch [12/20], Step [28378/47960], Loss: 3.8037, Accuracy: 100.0\n",
            "Epoch [12/20], Step [28478/47960], Loss: 5.5484, Accuracy: 0.0\n",
            "Epoch [12/20], Step [28578/47960], Loss: 4.4356, Accuracy: 0.0\n",
            "Epoch [12/20], Step [28678/47960], Loss: 4.8309, Accuracy: 0.0\n",
            "Epoch [13/20], Step [28876/47960], Loss: 0.4125, Accuracy: 100.0\n",
            "Epoch [13/20], Step [28976/47960], Loss: 0.9530, Accuracy: 100.0\n",
            "Epoch [13/20], Step [29076/47960], Loss: 0.9744, Accuracy: 0.0\n",
            "Epoch [13/20], Step [29176/47960], Loss: 1.8551, Accuracy: 0.0\n",
            "Epoch [13/20], Step [29276/47960], Loss: 0.7549, Accuracy: 100.0\n",
            "Epoch [13/20], Step [29376/47960], Loss: 15.8050, Accuracy: 0.0\n",
            "Epoch [13/20], Step [29476/47960], Loss: 0.7232, Accuracy: 100.0\n",
            "Epoch [13/20], Step [29576/47960], Loss: 1.3484, Accuracy: 100.0\n",
            "Epoch [13/20], Step [29676/47960], Loss: 2.3650, Accuracy: 0.0\n",
            "Epoch [13/20], Step [29776/47960], Loss: 0.6802, Accuracy: 100.0\n",
            "Epoch [13/20], Step [29876/47960], Loss: 6.7632, Accuracy: 100.0\n",
            "Epoch [13/20], Step [29976/47960], Loss: 6.1706, Accuracy: 0.0\n",
            "Epoch [13/20], Step [30076/47960], Loss: 4.5853, Accuracy: 0.0\n",
            "Epoch [13/20], Step [30176/47960], Loss: 5.4012, Accuracy: 0.0\n",
            "Epoch [13/20], Step [30276/47960], Loss: 1.3796, Accuracy: 100.0\n",
            "Epoch [13/20], Step [30376/47960], Loss: 1.4043, Accuracy: 100.0\n",
            "Epoch [13/20], Step [30476/47960], Loss: 4.9020, Accuracy: 0.0\n",
            "Epoch [13/20], Step [30576/47960], Loss: 2.4150, Accuracy: 100.0\n",
            "Epoch [13/20], Step [30676/47960], Loss: 1.4258, Accuracy: 100.0\n",
            "Epoch [13/20], Step [30776/47960], Loss: 3.7405, Accuracy: 100.0\n",
            "Epoch [13/20], Step [30876/47960], Loss: 5.5223, Accuracy: 0.0\n",
            "Epoch [13/20], Step [30976/47960], Loss: 4.4473, Accuracy: 0.0\n",
            "Epoch [13/20], Step [31076/47960], Loss: 5.2030, Accuracy: 0.0\n",
            "Epoch [14/20], Step [31274/47960], Loss: 0.4928, Accuracy: 100.0\n",
            "Epoch [14/20], Step [31374/47960], Loss: 1.0786, Accuracy: 0.0\n",
            "Epoch [14/20], Step [31474/47960], Loss: 0.7093, Accuracy: 0.0\n",
            "Epoch [14/20], Step [31574/47960], Loss: 2.0529, Accuracy: 0.0\n",
            "Epoch [14/20], Step [31674/47960], Loss: 3.4240, Accuracy: 0.0\n",
            "Epoch [14/20], Step [31774/47960], Loss: 10.4342, Accuracy: 0.0\n",
            "Epoch [14/20], Step [31874/47960], Loss: 0.1834, Accuracy: 100.0\n",
            "Epoch [14/20], Step [31974/47960], Loss: 1.3543, Accuracy: 100.0\n",
            "Epoch [14/20], Step [32074/47960], Loss: 2.3883, Accuracy: 0.0\n",
            "Epoch [14/20], Step [32174/47960], Loss: 1.2849, Accuracy: 100.0\n",
            "Epoch [14/20], Step [32274/47960], Loss: 10.5828, Accuracy: 100.0\n",
            "Epoch [14/20], Step [32374/47960], Loss: 6.0658, Accuracy: 0.0\n",
            "Epoch [14/20], Step [32474/47960], Loss: 4.6708, Accuracy: 0.0\n",
            "Epoch [14/20], Step [32574/47960], Loss: 4.7318, Accuracy: 100.0\n",
            "Epoch [14/20], Step [32674/47960], Loss: 1.2416, Accuracy: 100.0\n",
            "Epoch [14/20], Step [32774/47960], Loss: 1.3452, Accuracy: 100.0\n",
            "Epoch [14/20], Step [32874/47960], Loss: 4.8707, Accuracy: 0.0\n",
            "Epoch [14/20], Step [32974/47960], Loss: 2.4548, Accuracy: 100.0\n",
            "Epoch [14/20], Step [33074/47960], Loss: 1.4089, Accuracy: 100.0\n",
            "Epoch [14/20], Step [33174/47960], Loss: 3.7646, Accuracy: 100.0\n",
            "Epoch [14/20], Step [33274/47960], Loss: 5.4751, Accuracy: 0.0\n",
            "Epoch [14/20], Step [33374/47960], Loss: 4.5093, Accuracy: 100.0\n",
            "Epoch [14/20], Step [33474/47960], Loss: 4.8482, Accuracy: 0.0\n",
            "Epoch [15/20], Step [33672/47960], Loss: 1.4233, Accuracy: 100.0\n",
            "Epoch [15/20], Step [33772/47960], Loss: 1.4203, Accuracy: 0.0\n",
            "Epoch [15/20], Step [33872/47960], Loss: 1.8358, Accuracy: 0.0\n",
            "Epoch [15/20], Step [33972/47960], Loss: 2.1945, Accuracy: 0.0\n",
            "Epoch [15/20], Step [34072/47960], Loss: 3.6169, Accuracy: 0.0\n",
            "Epoch [15/20], Step [34172/47960], Loss: 4.3272, Accuracy: 0.0\n",
            "Epoch [15/20], Step [34272/47960], Loss: 0.3644, Accuracy: 100.0\n",
            "Epoch [15/20], Step [34372/47960], Loss: 0.4545, Accuracy: 100.0\n",
            "Epoch [15/20], Step [34472/47960], Loss: 2.4123, Accuracy: 0.0\n",
            "Epoch [15/20], Step [34572/47960], Loss: 1.2749, Accuracy: 100.0\n",
            "Epoch [15/20], Step [34672/47960], Loss: 7.4928, Accuracy: 100.0\n",
            "Epoch [15/20], Step [34772/47960], Loss: 6.1775, Accuracy: 0.0\n",
            "Epoch [15/20], Step [34872/47960], Loss: 4.4903, Accuracy: 0.0\n",
            "Epoch [15/20], Step [34972/47960], Loss: 5.4569, Accuracy: 0.0\n",
            "Epoch [15/20], Step [35072/47960], Loss: 1.3511, Accuracy: 100.0\n",
            "Epoch [15/20], Step [35172/47960], Loss: 0.3479, Accuracy: 100.0\n",
            "Epoch [15/20], Step [35272/47960], Loss: 2.3626, Accuracy: 100.0\n",
            "Epoch [15/20], Step [35372/47960], Loss: 3.1205, Accuracy: 0.0\n",
            "Epoch [15/20], Step [35472/47960], Loss: 1.1162, Accuracy: 100.0\n",
            "Epoch [15/20], Step [35572/47960], Loss: 3.8782, Accuracy: 100.0\n",
            "Epoch [15/20], Step [35672/47960], Loss: 5.5366, Accuracy: 0.0\n",
            "Epoch [15/20], Step [35772/47960], Loss: 4.3952, Accuracy: 0.0\n",
            "Epoch [15/20], Step [35872/47960], Loss: 4.8366, Accuracy: 100.0\n",
            "Epoch [16/20], Step [36070/47960], Loss: 0.4741, Accuracy: 100.0\n",
            "Epoch [16/20], Step [36170/47960], Loss: 0.4035, Accuracy: 100.0\n",
            "Epoch [16/20], Step [36270/47960], Loss: 0.9248, Accuracy: 0.0\n",
            "Epoch [16/20], Step [36370/47960], Loss: 0.9645, Accuracy: 0.0\n",
            "Epoch [16/20], Step [36470/47960], Loss: 1.9132, Accuracy: 0.0\n",
            "Epoch [16/20], Step [36570/47960], Loss: 4.1541, Accuracy: 0.0\n",
            "Epoch [16/20], Step [36670/47960], Loss: 1.3982, Accuracy: 100.0\n",
            "Epoch [16/20], Step [36770/47960], Loss: 0.0000, Accuracy: 100.0\n",
            "Epoch [16/20], Step [36870/47960], Loss: 5.7186, Accuracy: 0.0\n",
            "Epoch [16/20], Step [36970/47960], Loss: 1.3692, Accuracy: 100.0\n",
            "Epoch [16/20], Step [37070/47960], Loss: 4.1659, Accuracy: 100.0\n",
            "Epoch [16/20], Step [37170/47960], Loss: 6.1686, Accuracy: 0.0\n",
            "Epoch [16/20], Step [37270/47960], Loss: 3.9542, Accuracy: 0.0\n",
            "Epoch [16/20], Step [37370/47960], Loss: 7.8936, Accuracy: 0.0\n",
            "Epoch [16/20], Step [37470/47960], Loss: 1.4324, Accuracy: 100.0\n",
            "Epoch [16/20], Step [37570/47960], Loss: 1.4106, Accuracy: 100.0\n",
            "Epoch [16/20], Step [37670/47960], Loss: 4.9082, Accuracy: 0.0\n",
            "Epoch [16/20], Step [37770/47960], Loss: 4.5066, Accuracy: 0.0\n",
            "Epoch [16/20], Step [37870/47960], Loss: 1.4383, Accuracy: 100.0\n",
            "Epoch [16/20], Step [37970/47960], Loss: 3.7601, Accuracy: 100.0\n",
            "Epoch [16/20], Step [38070/47960], Loss: 5.5965, Accuracy: 0.0\n",
            "Epoch [16/20], Step [38170/47960], Loss: 4.6471, Accuracy: 0.0\n",
            "Epoch [16/20], Step [38270/47960], Loss: 5.1665, Accuracy: 0.0\n",
            "Epoch [17/20], Step [38468/47960], Loss: 1.3572, Accuracy: 0.0\n",
            "Epoch [17/20], Step [38568/47960], Loss: 1.3347, Accuracy: 100.0\n",
            "Epoch [17/20], Step [38668/47960], Loss: 0.8037, Accuracy: 0.0\n",
            "Epoch [17/20], Step [38768/47960], Loss: 1.1557, Accuracy: 0.0\n",
            "Epoch [17/20], Step [38868/47960], Loss: 3.1679, Accuracy: 0.0\n",
            "Epoch [17/20], Step [38968/47960], Loss: 4.3188, Accuracy: 0.0\n",
            "Epoch [17/20], Step [39068/47960], Loss: 1.3020, Accuracy: 100.0\n",
            "Epoch [17/20], Step [39168/47960], Loss: 1.0336, Accuracy: 100.0\n",
            "Epoch [17/20], Step [39268/47960], Loss: 6.3371, Accuracy: 0.0\n",
            "Epoch [17/20], Step [39368/47960], Loss: 1.2626, Accuracy: 100.0\n",
            "Epoch [17/20], Step [39468/47960], Loss: 5.5989, Accuracy: 100.0\n",
            "Epoch [17/20], Step [39568/47960], Loss: 6.2494, Accuracy: 0.0\n",
            "Epoch [17/20], Step [39668/47960], Loss: 2.5371, Accuracy: 100.0\n",
            "Epoch [17/20], Step [39768/47960], Loss: 5.6585, Accuracy: 0.0\n",
            "Epoch [17/20], Step [39868/47960], Loss: 1.3519, Accuracy: 100.0\n",
            "Epoch [17/20], Step [39968/47960], Loss: 1.3335, Accuracy: 100.0\n",
            "Epoch [17/20], Step [40068/47960], Loss: 4.9505, Accuracy: 0.0\n",
            "Epoch [17/20], Step [40168/47960], Loss: 2.6997, Accuracy: 100.0\n",
            "Epoch [17/20], Step [40268/47960], Loss: 0.9754, Accuracy: 100.0\n",
            "Epoch [17/20], Step [40368/47960], Loss: 3.7126, Accuracy: 100.0\n",
            "Epoch [17/20], Step [40468/47960], Loss: 5.6450, Accuracy: 0.0\n",
            "Epoch [17/20], Step [40568/47960], Loss: 4.5009, Accuracy: 0.0\n",
            "Epoch [17/20], Step [40668/47960], Loss: 5.2542, Accuracy: 100.0\n",
            "Epoch [18/20], Step [40866/47960], Loss: 0.6724, Accuracy: 100.0\n",
            "Epoch [18/20], Step [40966/47960], Loss: 1.3462, Accuracy: 100.0\n",
            "Epoch [18/20], Step [41066/47960], Loss: 0.5351, Accuracy: 100.0\n",
            "Epoch [18/20], Step [41166/47960], Loss: 0.7660, Accuracy: 100.0\n",
            "Epoch [18/20], Step [41266/47960], Loss: 3.4742, Accuracy: 0.0\n",
            "Epoch [18/20], Step [41366/47960], Loss: 15.2455, Accuracy: 0.0\n",
            "Epoch [18/20], Step [41466/47960], Loss: 0.6866, Accuracy: 100.0\n",
            "Epoch [18/20], Step [41566/47960], Loss: 0.3743, Accuracy: 100.0\n",
            "Epoch [18/20], Step [41666/47960], Loss: 5.4888, Accuracy: 0.0\n",
            "Epoch [18/20], Step [41766/47960], Loss: 1.2506, Accuracy: 100.0\n",
            "Epoch [18/20], Step [41866/47960], Loss: 5.7720, Accuracy: 100.0\n",
            "Epoch [18/20], Step [41966/47960], Loss: 6.5112, Accuracy: 0.0\n",
            "Epoch [18/20], Step [42066/47960], Loss: 3.4511, Accuracy: 0.0\n",
            "Epoch [18/20], Step [42166/47960], Loss: 5.3691, Accuracy: 0.0\n",
            "Epoch [18/20], Step [42266/47960], Loss: 1.4288, Accuracy: 100.0\n",
            "Epoch [18/20], Step [42366/47960], Loss: 0.2837, Accuracy: 100.0\n",
            "Epoch [18/20], Step [42466/47960], Loss: 4.8565, Accuracy: 0.0\n",
            "Epoch [18/20], Step [42566/47960], Loss: 4.6401, Accuracy: 0.0\n",
            "Epoch [18/20], Step [42666/47960], Loss: 1.4151, Accuracy: 100.0\n",
            "Epoch [18/20], Step [42766/47960], Loss: 3.7448, Accuracy: 100.0\n",
            "Epoch [18/20], Step [42866/47960], Loss: 5.4054, Accuracy: 0.0\n",
            "Epoch [18/20], Step [42966/47960], Loss: 4.4489, Accuracy: 0.0\n",
            "Epoch [18/20], Step [43066/47960], Loss: 5.1779, Accuracy: 0.0\n",
            "Epoch [19/20], Step [43264/47960], Loss: 1.3394, Accuracy: 100.0\n",
            "Epoch [19/20], Step [43364/47960], Loss: 2.3610, Accuracy: 0.0\n",
            "Epoch [19/20], Step [43464/47960], Loss: 1.8704, Accuracy: 0.0\n",
            "Epoch [19/20], Step [43564/47960], Loss: 1.4141, Accuracy: 0.0\n",
            "Epoch [19/20], Step [43664/47960], Loss: 2.2953, Accuracy: 0.0\n",
            "Epoch [19/20], Step [43764/47960], Loss: 5.5315, Accuracy: 0.0\n",
            "Epoch [19/20], Step [43864/47960], Loss: 0.9557, Accuracy: 0.0\n",
            "Epoch [19/20], Step [43964/47960], Loss: 0.0000, Accuracy: 100.0\n",
            "Epoch [19/20], Step [44064/47960], Loss: 2.2597, Accuracy: 0.0\n",
            "Epoch [19/20], Step [44164/47960], Loss: 1.2716, Accuracy: 100.0\n",
            "Epoch [19/20], Step [44264/47960], Loss: 4.1595, Accuracy: 100.0\n",
            "Epoch [19/20], Step [44364/47960], Loss: 7.1185, Accuracy: 0.0\n",
            "Epoch [19/20], Step [44464/47960], Loss: 4.6667, Accuracy: 0.0\n",
            "Epoch [19/20], Step [44564/47960], Loss: 5.7625, Accuracy: 0.0\n",
            "Epoch [19/20], Step [44664/47960], Loss: 1.3409, Accuracy: 100.0\n",
            "Epoch [19/20], Step [44764/47960], Loss: 1.3227, Accuracy: 100.0\n",
            "Epoch [19/20], Step [44864/47960], Loss: 4.9535, Accuracy: 0.0\n",
            "Epoch [19/20], Step [44964/47960], Loss: 4.6947, Accuracy: 0.0\n",
            "Epoch [19/20], Step [45064/47960], Loss: 1.0306, Accuracy: 100.0\n",
            "Epoch [19/20], Step [45164/47960], Loss: 3.7864, Accuracy: 100.0\n",
            "Epoch [19/20], Step [45264/47960], Loss: 5.6363, Accuracy: 0.0\n",
            "Epoch [19/20], Step [45364/47960], Loss: 4.4918, Accuracy: 0.0\n",
            "Epoch [19/20], Step [45464/47960], Loss: 6.8078, Accuracy: 0.0\n",
            "Epoch [20/20], Step [45662/47960], Loss: 1.3719, Accuracy: 100.0\n",
            "Epoch [20/20], Step [45762/47960], Loss: 0.9276, Accuracy: 0.0\n",
            "Epoch [20/20], Step [45862/47960], Loss: 1.1812, Accuracy: 0.0\n",
            "Epoch [20/20], Step [45962/47960], Loss: 0.3963, Accuracy: 100.0\n",
            "Epoch [20/20], Step [46062/47960], Loss: 2.2929, Accuracy: 0.0\n",
            "Epoch [20/20], Step [46162/47960], Loss: 4.2473, Accuracy: 0.0\n",
            "Epoch [20/20], Step [46262/47960], Loss: 0.7716, Accuracy: 0.0\n",
            "Epoch [20/20], Step [46362/47960], Loss: 0.0200, Accuracy: 100.0\n",
            "Epoch [20/20], Step [46462/47960], Loss: 7.3973, Accuracy: 0.0\n",
            "Epoch [20/20], Step [46562/47960], Loss: 0.7800, Accuracy: 100.0\n",
            "Epoch [20/20], Step [46662/47960], Loss: 6.0631, Accuracy: 100.0\n",
            "Epoch [20/20], Step [46762/47960], Loss: 6.0978, Accuracy: 0.0\n",
            "Epoch [20/20], Step [46862/47960], Loss: 4.5899, Accuracy: 0.0\n",
            "Epoch [20/20], Step [46962/47960], Loss: 5.3231, Accuracy: 0.0\n",
            "Epoch [20/20], Step [47062/47960], Loss: 1.3420, Accuracy: 100.0\n",
            "Epoch [20/20], Step [47162/47960], Loss: 1.4239, Accuracy: 100.0\n",
            "Epoch [20/20], Step [47262/47960], Loss: 4.8267, Accuracy: 0.0\n",
            "Epoch [20/20], Step [47362/47960], Loss: 4.2190, Accuracy: 0.0\n",
            "Epoch [20/20], Step [47462/47960], Loss: 1.4548, Accuracy: 100.0\n",
            "Epoch [20/20], Step [47562/47960], Loss: 3.7380, Accuracy: 100.0\n",
            "Epoch [20/20], Step [47662/47960], Loss: 5.1682, Accuracy: 0.0\n",
            "Epoch [20/20], Step [47762/47960], Loss: 4.3615, Accuracy: 0.0\n",
            "Epoch [20/20], Step [47862/47960], Loss: 5.0995, Accuracy: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIwLxZThH3qS",
        "colab_type": "code",
        "outputId": "7a03e583-dd85-46bf-b05a-cf4e85075cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "source": [
        "plt.plot(ells['accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(ells['loss'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEGCAYAAACtqQjWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMcb49P8BeDozCZV4QnAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvNy0SbLQg62",
        "colab_type": "text"
      },
      "source": [
        "### Training Script: `run_script.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q5CPAt0ll8X",
        "colab_type": "code",
        "outputId": "4f207675-a962-4b18-8326-d142bc15919c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "def train(net, optimizer, criterion, epochs, batch, exp_name):\n",
        "    model = net.to(device)\n",
        "    total_step = len(dl)\n",
        "    overall_step = 0\n",
        "    losses = []\n",
        "    kl_loss = []\n",
        "    mseX_loss = []\n",
        "    mseY_loss = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total = 0\n",
        "        running_loss = 0.0\n",
        "        kl_running = 0.0\n",
        "        mseX_running = 0.0\n",
        "        mseY_running = 0.0\n",
        "\n",
        "        for i, X in enumerate(dl):\n",
        "            t0 = X[0].float().to(device)\n",
        "            tk = X[1].float().to(device)\n",
        "\n",
        "            xhat, yhat, z, z_mean, z_logvar = model.forward(t0,tk)\n",
        "            \n",
        "            loss, MSE_X, MSE_Y, KLD = criterion(xhat,t0, yhat, tk, z_mean, z_logvar)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            kl_running += KLD.item()\n",
        "            mseX_running += MSE_X.item()\n",
        "            mseY_running += MSE_Y.item()\n",
        "            total += batch\n",
        "\n",
        "            overall_step += 1\n",
        "\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, overall_step, total_step, loss.item()))\n",
        "            if i == 25:\n",
        "                break\n",
        "\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                print(\"Current learning rate is: {}\".format(param_group['lr']))\n",
        "            chpt_path = base+'checkpoints/'+exp_name+'.pt'\n",
        "            torch.save(model.state_dict(), chpt_path)\n",
        "\n",
        "        losses.append(running_loss/total)     \n",
        "        kl_loss.append(kl_running/total)\n",
        "        mseX_loss.append(mseX_running/total)\n",
        "        mseY_loss.append(mseY_running/total)  \n",
        "    \n",
        "    ells = {'elbo':losses,\n",
        "            'kl':kl_loss,\n",
        "            'mseX':mseX_loss,\n",
        "            'mseY':mseY_loss}\n",
        "\n",
        "    with open(base+'logs/'+exp_name+'_losses.pickle', 'wb') as f:\n",
        "        pickle.dump(ells, f)\n",
        "\n",
        "    left = []\n",
        "    right = []\n",
        "    recon_left = []\n",
        "    recon_right = []\n",
        "    for i, X in enumerate(dl):\n",
        "        model.eval()\n",
        "        t0 = X[0].float().to(device)\n",
        "        tk = X[1].float().to(device)\n",
        "        u = X[2].float().to(device)\n",
        "\n",
        "        #Forward Pass\n",
        "        xhat, yhat, z, z_mean, z_stdev = model.forward(t0,tk)\n",
        "\n",
        "        left_ = t0.cpu().squeeze().numpy()\n",
        "        right_ = tk.cpu().squeeze().numpy()\n",
        "        xhat_ = xhat.cpu().detach().squeeze().numpy()\n",
        "        yhat_ = yhat.cpu().detach().squeeze().numpy()\n",
        "\n",
        "        left.append(left_)\n",
        "        right.append(right_)\n",
        "        recon_left.append(xhat_)\n",
        "        recon_right.append(yhat_)\n",
        "        if i == 40:\n",
        "            break\n",
        "    left = np.asarray(left)\n",
        "    right = np.asarray(right)\n",
        "    recon_left = np.asarray(recon_left)\n",
        "    recon_right = np.asarray(recon_right)\n",
        "\n",
        "    plt.plot(losses, label='ELBO')\n",
        "    plt.plot(kl_loss, label='KL')\n",
        "    plt.plot(mseX_loss, label='MSE')\n",
        "    plt.plot(mseY_loss, label='MSE')\n",
        "    plt.title('Train loss')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.subplot(221)\n",
        "    plt.imshow(left[0], cmap = 'gray')\n",
        "    plt.subplot(222)\n",
        "    plt.imshow(right[0], cmap = 'gray')\n",
        "    plt.subplot(223)\n",
        "    plt.imshow(recon_left[0], cmap = 'gray')\n",
        "    plt.subplot(224)\n",
        "    plt.imshow(recon_right[0], cmap = 'gray')\n",
        "    plt.figure()\n",
        "    plt.subplot(221)\n",
        "    plt.imshow(left[30], cmap = 'gray')\n",
        "    plt.subplot(222)\n",
        "    plt.imshow(right[30], cmap = 'gray')\n",
        "    plt.subplot(223)\n",
        "    plt.imshow(recon_left[30], cmap = 'gray')\n",
        "    plt.subplot(224)\n",
        "    plt.imshow(recon_right[30], cmap = 'gray')\n",
        "\n",
        "\n",
        "#Run from here\n",
        "exp_name = 'siamese_single_test'\n",
        "model = siameseCVAE(batch=batch)\n",
        "# checkpoint = torch.load('/content/drive/My Drive/Colab_Notebooks/ESE546_DL_Colab/project/checkpoints/siamese_chpt.pt')\n",
        "# model.load_state_dict(checkpoint) \n",
        "\n",
        "epochs = 1\n",
        "criterion = ELBO_loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-3)\n",
        "\n",
        "train(model, optimizer, criterion, epochs, batch, exp_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [1/2398], Loss: 68553.3359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6ede766535af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-6ede766535af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, optimizer, criterion, epochs, batch, exp_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 938.00 MiB (GPU 0; 15.90 GiB total capacity; 13.88 GiB already allocated; 845.88 MiB free; 507.41 MiB cached)"
          ]
        }
      ]
    }
  ]
}